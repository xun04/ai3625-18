[
  {
    "timestamp": "2025-10-22T03:45:35.087Z",
    "event_type": "user_query",
    "user_id": "jiadisu7@gmail.com",
    "session_id": "43515b75-14c3-42dd-9420-b3fee5ce30d3",
    "query": "This is the SII Cli. We are setting up the context for our chat.\n  Today's date is Wednesday, October 22, 2025.\n  My operating system is: linux\n  I'm currently working in the directory: /data2/jdsu/projects/OmniAvatar\n  Showing up to 20 items (files + folders). Folders or files indicated with ... contain more items not shown, were ignored, or the display limit (20 items) was reached.\n\n/data2/jdsu/projects/OmniAvatar/\nâ”œâ”€â”€â”€.gitignore\nâ”œâ”€â”€â”€frontend_server.py\nâ”œâ”€â”€â”€LICENSE.txt\nâ”œâ”€â”€â”€merge_av.sh\nâ”œâ”€â”€â”€pipelined_websocket_streaming_server.py\nâ”œâ”€â”€â”€qwen_omni.py\nâ”œâ”€â”€â”€README.md\nâ”œâ”€â”€â”€requirements.txt\nâ”œâ”€â”€â”€run_backend.sh\nâ”œâ”€â”€â”€start_causal_server.sh\nâ”œâ”€â”€â”€.claude/\nâ”œâ”€â”€â”€.git/...\nâ”œâ”€â”€â”€.sii/\nâ”œâ”€â”€â”€assets/\nâ”œâ”€â”€â”€configs/\nâ”œâ”€â”€â”€demo_out/\nâ”œâ”€â”€â”€examples/\nâ”œâ”€â”€â”€OmniAvatar/\nâ”œâ”€â”€â”€qwen-omni-deploy/\nâ”œâ”€â”€â”€scripts/\nâ””â”€â”€â”€...",
    "metadata": {
      "cli_version": "0.0.33",
      "platform": "linux",
      "working_directory": "/data2/jdsu/projects/OmniAvatar"
    }
  },
  {
    "timestamp": "2025-10-22T03:45:35.093Z",
    "event_type": "assistant_response",
    "user_id": "jiadisu7@gmail.com",
    "session_id": "43515b75-14c3-42dd-9420-b3fee5ce30d3",
    "response": "Got it. Thanks for the context!",
    "model": "anthropic/claude-sonnet-4",
    "metadata": {
      "cli_version": "0.0.33",
      "platform": "linux"
    }
  },
  {
    "timestamp": "2025-10-22T03:45:35.115Z",
    "event_type": "user_query",
    "user_id": "jiadisu7@gmail.com",
    "session_id": "43515b75-14c3-42dd-9420-b3fee5ce30d3",
    "query": "åŒæ ·çš„ä¸€ä¸ªDiTï¼Œå¯¹ä¸€ä¸ªé•¿åº¦ä¸ºLçš„åºåˆ—åšå»å™ªæ‰€éœ€è¦çš„æ—¶é—´ä¸ºT_{L}ï¼Œå¯¹ä¸€ä¸ªé•¿åº¦ä¸º2Lçš„åºåˆ—åšå»å™ªæ‰€éœ€è¦çš„æ—¶é—´ä¸ºT_{2L}ï¼Œåœ¨æ˜¾å­˜ä¸å—é™çš„æƒ…å†µä¸‹ï¼ŒT_{2L}å’Œ2*T_{L}çš„å…³ç³»æ˜¯æ€ä¹ˆæ ·çš„ï¼Ÿ",
    "metadata": {
      "cli_version": "0.0.33",
      "platform": "linux",
      "working_directory": "/data2/jdsu/projects/OmniAvatar"
    }
  },
  {
    "timestamp": "2025-10-22T03:45:39.766Z",
    "event_type": "assistant_response",
    "user_id": "jiadisu7@gmail.com",
    "session_id": "43515b75-14c3-42dd-9420-b3fee5ce30d3",
    "response": "I'll analyze the relationship between denoising time for different sequence lengths in a DiT (Diffusion Transformer) model.",
    "model": "anthropic/claude-sonnet-4",
    "metadata": {
      "cli_version": "0.0.33",
      "platform": "linux"
    }
  },
  {
    "timestamp": "2025-10-22T03:45:41.197Z",
    "event_type": "assistant_response",
    "user_id": "jiadisu7@gmail.com",
    "session_id": "43515b75-14c3-42dd-9420-b3fee5ce30d3",
    "response": "",
    "model": "anthropic/claude-sonnet-4",
    "metadata": {
      "cli_version": "0.0.33",
      "platform": "linux"
    }
  },
  {
    "timestamp": "2025-10-22T03:45:41.197Z",
    "event_type": "tool_call",
    "user_id": "jiadisu7@gmail.com",
    "session_id": "43515b75-14c3-42dd-9420-b3fee5ce30d3",
    "tool_name": "sii_web_search",
    "tool_args": "{\"query\":\"DiT Diffusion Transformer computational complexity sequence length scaling\"}",
    "tool_call_id": "call_tooluvrtx01Q97qa92Ki",
    "metadata": {
      "tool_type": "function"
    }
  },
  {
    "timestamp": "2025-10-22T03:45:41.223Z",
    "event_type": "tool_result",
    "user_id": "jiadisu7@gmail.com",
    "session_id": "43515b75-14c3-42dd-9420-b3fee5ce30d3",
    "tool_result": "Search results for \"DiT Diffusion Transformer computational complexity sequence length scaling\":\n\n1. **Diffusion Transformer (DiT) Models: A Beginner's Guide**\n   Scaling the model size involves increasing the complexity of the model, typically by adding more layers or increasing the number of neurons in ...\n   Source: https://encord.com/blog/diffusion-models-with-transformers/\n\n2. **Scaling Laws for Diffusion Transformers**\n   In this work, we characterize the scaling behavior of diffusion models for text-to-image synthesis, resulting in the explicit scaling laws of DiT for the first ...\n   Source: https://arxiv.org/html/2410.08184v1\n\n3. **EC-DIT: Scaling Diffusion Transformers with Adaptive ...**\n   by H Sun Â· Cited by 5 â€” We develop a new family of Mixture-of-Experts (MoE) models (EC-DIT) for diffusion transformers with expert-choice routing.\n   Source: https://openreview.net/forum?id=PxlfzEePC0\n\n4. **Scalable Diffusion Models with Transformers**\n   by W Peebles Â· 2023 Â· Cited by 3886 â€” More specifically, we study the scaling behavior of trans- formers with respect to network complexity vs. sample quality. We show that by constructing and ...\n   Source: https://openaccess.thecvf.com/content/ICCV2023/papers/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.pdf\n\n5. **D2iT: Dynamic Diffusion Transformer for Accurate Image ...**\n   The key to the success of DiT lies in its spatial compression, which significantly reduces the image sequence length. This reduction is ...\n   Source: https://arxiv.org/html/2504.09454v1\n\n\nSources:\n[1] Diffusion Transformer (DiT) Models: A Beginner's Guide (https://encord.com/blog/diffusion-models-with-transformers/)\n[2] Scaling Laws for Diffusion Transformers (https://arxiv.org/html/2410.08184v1)\n[3] EC-DIT: Scaling Diffusion Transformers with Adaptive ... (https://openreview.net/forum?id=PxlfzEePC0)\n[4] Scalable Diffusion Models with Transformers (https://openaccess.thecvf.com/content/ICCV2023/papers/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.pdf)\n[5] D2iT: Dynamic Diffusion Transformer for Accurate Image ... (https://arxiv.org/html/2504.09454v1)",
    "tool_call_id": "call_tooluvrtx01Q97qa92Ki"
  },
  {
    "timestamp": "2025-10-22T03:47:45.441Z",
    "event_type": "assistant_response",
    "user_id": "jiadisu7@gmail.com",
    "session_id": "43515b75-14c3-42dd-9420-b3fee5ce30d3",
    "response": "",
    "model": "anthropic/claude-sonnet-4",
    "metadata": {
      "cli_version": "0.0.33",
      "platform": "linux"
    }
  },
  {
    "timestamp": "2025-10-22T03:47:45.441Z",
    "event_type": "tool_call",
    "user_id": "jiadisu7@gmail.com",
    "session_id": "43515b75-14c3-42dd-9420-b3fee5ce30d3",
    "tool_name": "sii_web_fetch",
    "tool_args": "{\"prompt\":\"Analyze the computational complexity of Diffusion Transformers (DiT) with respect to sequence length scaling from https://arxiv.org/html/2410.08184v1 and https://openaccess.thecvf.com/content/ICCV2023/papers/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.pdf. Focus on how denoising time scales with sequence length.\"}",
    "tool_call_id": "call_tooluvrtx01QbjqVZ5iV",
    "metadata": {
      "tool_type": "function"
    }
  },
  {
    "timestamp": "2025-10-22T03:47:45.472Z",
    "event_type": "tool_result",
    "user_id": "jiadisu7@gmail.com",
    "session_id": "43515b75-14c3-42dd-9420-b3fee5ce30d3",
    "tool_result": "Successfully fetched content from 1 URL(s):\n\nUser request: \"Analyze the computational complexity of Diffusion Transformers (DiT) with respect to sequence length scaling from https://arxiv.org/html/2410.08184v1 and https://openaccess.thecvf.com/content/ICCV2023/papers/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.pdf. Focus on how denoising time scales with sequence length.\"\n\nFetched content:\n\n--- Content from https://arxiv.org/html/2410.08184v1 ---\nScaling Laws for Diffusion Transformers\n\nZhengyang Liang1, Hao He2, Ceyuan Yang3111Equal Supervision., Bo Dai4, 1111Equal Supervision.Â Â 222Corresponding Author.\n1Shanghai Artificial Intelligence Laboratory 2The Chinese University of Hong Kong\n3ByteDance 4The University of Hong Kong\n\n\nAbstract\n\nDiffusion transformers (DiT) have already achieved appealing synthesis and scaling properties in content recreation, e.g., image and video generation. However, scaling laws of DiT are less explored, which usually offer precise predictions regarding optimal model size and data requirements given a specific compute budget. Therefore, experiments across a broad range of compute budgets, from 1e17 to 6e18 FLOPs are conducted to confirm the existence of scaling laws in DiT for the first time. Concretely, the loss of pretraining DiT also follows a power-law relationship with the involved compute. Based on the scaling law, we can not only determine the optimal model size and required data but also accurately predict the text-to-image generation loss given a model with 1B parameters and a compute budget of 1e21 FLOPs. Additionally, we also demonstrate that the trend of pretraining loss matches the generation performances (e.g., FID), even across various datasets, which complements the mapping from compute to synthesis quality and thus provides a predictable benchmark that assesses model performance and data quality at a reduced cost.\n\n\n1 Introduction\n\nScaling laws in large language models (LLMs) (Kaplan etÂ al., 2020; Hestness etÂ al., 2017; Henighan etÂ al., 2020; Hoffmann etÂ al., 2022) have been widely observed and validated, suggesting that pretraining performance follows a power-law relationship with the compute Cğ¶Citalic_C. The actual compute could be roughly calculated as C=6â¢Nâ¢Dğ¶6ğ‘ğ·C=6NDitalic_C = 6 italic_N italic_D, where Nğ‘Nitalic_N is the model size and Dğ·Ditalic_D is the data quantity. Therefore, determining the scaling law helps us make informed decisions about resource allocation to maximize computational efficiency, namely, figure out the optimal balance between model size and training data (i.e., the optimal model and data scale) given a fixed compute budget. However, scaling laws in diffusion models remain less explored.\n\nThe scalability has already been demonstrated in diffusion models, especially for diffusion transformers (DiT). Specifically, several prior works (Mei etÂ al., 2024; Li etÂ al., 2024) reveal that larger models always result in better visual quality and improved text-image alignment. However, the scaling property of diffusion transformers is clearly observed but not accurately predicted. Besides, the absence of explicit scaling laws also hinders a comprehensive understanding of how training budget relate to model size, data quantity, and loss. As a result, we cannot determine accordingly the optimal model and data sizes for a given compute budget and accurately predict training loss. Instead, heuristic configuration searches of models and data are required, which are costly and challenging to ensure optimal balance.\n\nIn this work, we characterize the scaling behavior of diffusion models for text-to-image synthesis, resulting in the explicit scaling laws of DiT for the first time. To investigate the explicit relationship between pretraining loss and compute, a wide range of compute budgets from 1e17 to 6e18 FLOPs are used. Models ranging from 1M to 1B are pretrained under given compute budgets. As shown in Fig.Â 1, for each compute budget, we can fit a parabola and extract an optimal point that corresponds to the optimal model size and consumed data under that specific compute constraint. Using these optimal configurations, we derive scaling laws by fitting a power-law relationship between compute budgets, model size, consumed data, and training loss. To evaluate the derived scaling laws, we extrapolate the compute budget to 1.5e21 FLOPs that results in the compute-optimal model size (approximately 1B parameters) and the corresponding data size. Therefore, a 1B-parameter model is trained under this budget and the final loss matches our prediction, demonstrating the effectiveness and accuracy of our scaling laws.\n\nTo make the best use of the scaling laws, we demonstrate that the generation performances (e.g., FID (FrÃ©chet Inception Distance)) also match the trend of pretraining loss. Namely, the synthesis quality also follows the power-law relationship with the compute budget, making it predictable. More importantly, this observation is transferable across various datasets. We conduct additional experiments on the COCO validation set (Lin etÂ al., 2014), and the same scaling patterns hold, even when tested on out-of-domain data. Accordingly, scaling laws could serve as a predictable benchmark where we can assess the quality of both models and datasets at a significantly reduced computational cost, enabling efficient evaluation and optimization of the model training process.\n\nTo summarize, we at first confirm the presence of scaling laws in diffusion transformers during training, revealing a clear power-law relationship between compute budget and training losses. Next, we establish a connection between pretraining loss and synthesis evaluation metrics. Finally, We conducted preliminary experiments that demonstrate the potential of using scaling laws to evaluate both data and model performance. By conducting scaling experiments at a relatively low cost, we can assess and validate the effectiveness of different configurations based on the fitted power-law coefficients.\n\n\n2 Related Work\n\nDiffusion Transformers\n\nTransformers have become the de facto model architecture in language modeling (Radford, 2018; Radford etÂ al., 2019; Devlin, 2018), and they have also achieved significant success in computer vision tasks (Dosovitskiy etÂ al., 2020; He etÂ al., 2022). Recently, Transformers have been introduced into diffusion models (Peebles & Xie, 2023), where images are divided into patches (tokens), and the diffusion process is learned on these tokens. Additional conditions, such as timestep and text, are incorporated into the network via cross-attention (Chen etÂ al., 2023), Adaptive Normalization (Perez etÂ al., 2018), or by concatenating them with image tokens (Bao etÂ al., 2023). Zheng etÂ al. (2023) proposed masked transformers to reduce training costs, while Lu etÂ al. (2024) introduced techniques for unrestricted resolution generation. Diffusion Transformers (DiTs) inherit the scalability, efficiency, and high capacity of Transformer architectures, positioning them as a promising backbone for diffusion models. Motivated by this scalability, we investigate the scaling laws governing these models in this work. To ensure robust and clear conclusions, we adopt a vanilla Transformer design (Vaswani, 2017), using a concatenation of image, text, and time tokens as input to the models.\n\nScaling Laws\n\nScaling laws (Hestness etÂ al., 2017) have been fundamental in understanding the performance of neural networks as they scale in size and data. This concept has been validated across several large pretraining models (Dubey etÂ al., 2024; Bi etÂ al., 2024; Achiam etÂ al., 2023). Kaplan etÂ al. (2020); Henighan etÂ al. (2020) were the first to formalize scaling laws in language models and extend them to autoregressive generative models, demonstrating that model performance scales predictably with increases in model size and dataset quantity. Hoffmann etÂ al. (2022) further highlighted the importance of balancing model size and dataset size to achieve optimal performance. In the context of diffusion models, prior works (Mei etÂ al., 2024; Li etÂ al., 2024) have empirically demonstrated their scaling properties, showing that larger compute budgets generally result in better models. These studies also compared the scaling behavior of various model architectures and explored sampling efficiency. However, no previous works provide an explicit formulation of scaling laws for diffusion transformers to capture the relationship between compute budget, model size, data, and loss. In this paper, we aim to address this gap by systematically investigating the scaling behavior of diffusion transformers (DiTs), offering a more comprehensive understanding of their scaling properties.\n\n\n3 method\n\n\n3.1 Basic Settings\n\nUnless otherwise stated, all experiments in this paper adhere to the same basic settings. While the training techniques and strategies employed may not be optimal, they primarily affect the scaling coefficients rather than the scaling trends. In this section, we outline the critical settings used in our experiments. Additional details are provided in Appendix C.\n\nDiffusion Formulation\n\nAll experiments are conducted using the Rectified Flow (RF) formulation (Liu etÂ al., 2022; Lipman etÂ al., 2022; Albergo etÂ al., 2023) with ğ¯ğ¯\\mathbf{v}bold_v-prediction. For timestep sampling, we adopt the Logit-Normal (LN) Sampling scheduler Ï€lâ¢nâ¢(t;m,s)subscriptğœ‹ğ‘™ğ‘›ğ‘¡ğ‘šğ‘ \\pi_{ln}(t;m,s)italic_Ï€ start_POSTSUBSCRIPT italic_l italic_n end_POSTSUBSCRIPT ( italic_t ; italic_m , italic_s ), as proposed in Esser etÂ al. (2024). A detailed ablation study of this choice can be found in Appendix F.1.\n\nModels & Dataset\n\nAs noted in Kaplan etÂ al. (2020), the model design does not significantly impact scaling trends unless the architecture is extremely shallow or slim. We employ a vanilla transformer architecture (Vaswani, 2017) with minimal modifications. The text, image, and timestep tokens are concatenated as input (in-context conditioning (Peebles & Xie, 2023)). A dataset of 108 million image-text pairs is randomly sampled from Laion-Aesthetic (Schuhmann etÂ al., 2022). This subset is then re-captioned using LLAVA 1.5 (Liu etÂ al., 2024). A validation set consisting of 1 million pairs is created from this subset. Further details can be found in Appendix C.1 and C.2.\n\nScaling Metrics\n\nA natural question arises when investigating scaling laws during training: What metrics should be selected to observe scaling behavior? In the context of Large Language Models (LLMs), the standard approach is autoregressive training (Radford, 2018; Radford etÂ al., 2019), where the model is trained to predict the next token in a sequence, directly optimizing the likelihood of the training data. This has proven to be a reliable method for measuring model performance as the compute budget scales up. Inspired by this approach, we extend the concept of scaling laws to diffusion models, using loss and likelihood as our key metrics.\n\nLoss is the primary metric chosen to observe scaling behavior during training. Unlike autoregressive models, diffusion models do not directly optimize likelihood. Instead, the objective is to match a time-conditioned velocity field (Ma etÂ al., 2024; Liu etÂ al., 2022; Lipman etÂ al., 2022; Albergo etÂ al., 2023). Specifically, the velocity ğ¯â¢(xt,t)ğ¯subscriptğ‘¥ğ‘¡ğ‘¡\\mathbf{v}(x_{t},t)bold_v ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) at timestep tğ‘¡titalic_t is defined as:\n\nğ¯â¢(xt,t)=xtâ€²=Î±tâ€²â¢x0+Î²tâ€²â¢Ïµ,ğ¯subscriptğ‘¥ğ‘¡ğ‘¡superscriptsubscriptğ‘¥ğ‘¡â€²superscriptsubscriptğ›¼ğ‘¡â€²subscriptğ‘¥0superscriptsubscriptğ›½ğ‘¡â€²italic-Ïµ\\displaystyle\\mathbf{v}(x_{t},t)=x_{t}^{\\prime}=\\alpha_{t}^{\\prime}x_{0}+\\beta%   (1)\n_{t}^{\\prime}\\epsilon,bold_v ( italic_x start_POSTSUBSCRIPT\nitalic_t end_POSTSUBSCRIPT , italic_t ) = italic_x\nstart_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT\nstart_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = italic_Î±\nstart_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT\nstart_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT italic_x\nstart_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_Î²\nstart_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT\nstart_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT italic_Ïµ ,\n\nwhere x0subscriptğ‘¥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT represents the original data and Ïµitalic-Ïµ\\epsilonitalic_Ïµ denotes the noise. Here, the prime symbol â€² indicates the derivative with respect to time tğ‘¡titalic_t. In the rectified flow framework, the coefficients Î±tsubscriptğ›¼ğ‘¡\\alpha_{t}italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and Î²tsubscriptğ›½ğ‘¡\\beta_{t}italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are defined as Î±t=1âˆ’t,Î²t=tformulae-sequencesubscriptğ›¼ğ‘¡1ğ‘¡subscriptğ›½ğ‘¡ğ‘¡\\alpha_{t}=1-t,\\beta_{t}=titalic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 - italic_t , italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_t. Thus, the velocity ğ¯ğ¯\\mathbf{v}bold_v can be further simplified as:\n\nğ¯â¢(xt,t)=âˆ’x0+Ïµ.ğ¯subscriptğ‘¥ğ‘¡ğ‘¡subscriptğ‘¥0italic-Ïµ\\displaystyle\\mathbf{v}(x_{t},t)=-x_{0}+\\epsilon.bold_v   (2)\n( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ,\nitalic_t ) = - italic_x start_POSTSUBSCRIPT 0\nend_POSTSUBSCRIPT + italic_Ïµ .\n\nThe corresponding loss function is expressed in terms of the expected value:\n\nâ„’â¢(Î¸,ğ±,t)=ğ”¼tâˆ¼[1,T],Ïµâˆ¼ğ’©â¢(0,I)â¢[â€–ğ¯Î¸â¢(xt,t)+x0âˆ’Ïµâ€–2]â‰ˆ1Nâ¢âˆ‘i=1N[â€–ğ¯Î¸â¢(xt,t)+x0âˆ’Ïµâ€–2].â„’ğœƒğ±ğ‘¡subscriptğ”¼formulae-sequencesimilar-toğ‘¡1ğ‘‡similar-toitalic-Ïµğ’©0Idelimited-[]superscriptnormsubscriptğ¯ğœƒsubscriptğ‘¥ğ‘¡ğ‘¡subscriptğ‘¥0italic-Ïµ21ğ‘superscriptsubscriptğ‘–1ğ‘delimited-[]superscriptnormsubscriptğ¯ğœƒsubscriptğ‘¥ğ‘¡ğ‘¡subscriptğ‘¥0italic-Ïµ2\\displaystyle\\mathcal{L}(\\theta,\\mathbf{x},t)=\\mathbb{E}_{t\\sim[1,T],\\epsilon%   (3)\n\\sim\\mathcal{N}(0,\\emph{I})}\\left[\\left\\|\\mathbf{v}_{\\theta}(x_{t},t)+x_{0}-%\n\\epsilon\\right\\|^{2}\\right]\\approx\\frac{1}{N}\\sum_{i=1}^{N}\\left[\\left\\|%\n\\mathbf{v}_{\\theta}(x_{t},t)+x_{0}-\\epsilon\\right\\|^{2}\\right].caligraphic_L\n( italic_Î¸ , bold_x , italic_t ) = blackboard_E\nstart_POSTSUBSCRIPT italic_t âˆ¼ [ 1 , italic_T ] , italic_Ïµ âˆ¼\ncaligraphic_N ( 0 , I ) end_POSTSUBSCRIPT [ âˆ¥ bold_v\nstart_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_x\nstart_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t )\n+ italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT -\nitalic_Ïµ âˆ¥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] â‰ˆ\ndivide start_ARG 1 end_ARG start_ARG italic_N end_ARG âˆ‘\nstart_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT\nstart_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT [ âˆ¥\nbold_v start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT (\nitalic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ,\nitalic_t ) + italic_x start_POSTSUBSCRIPT 0\nend_POSTSUBSCRIPT - italic_Ïµ âˆ¥ start_POSTSUPERSCRIPT 2\nend_POSTSUPERSCRIPT ] .\n\nThe training loss is estimated using a Monte Carlo method, which involves timesteps and noise sampling. The stochasticity inherent in this process can cause significant fluctuations, which are mitigated by employing a larger batch size of 1024 and applying Exponential Moving Average (EMA) smoothing. In our experiments, we set Î±EMA=0.9subscriptğ›¼EMA0.9\\alpha_{\\text{EMA}}=0.9italic_Î± start_POSTSUBSCRIPT EMA end_POSTSUBSCRIPT = 0.9, which is found to produce stable results. A detailed ablation study on the choice of EMA coefficients is provided in Appendix F.2. This smoothing procedure helps reduce variance and provides clearer insights into training dynamics.\n\nIn addition to the training loss, validation loss is also computed on the COCO 2014 dataset (Lin etÂ al., 2014). To ensure consistency with the training loss, timesteps are sampled using the LN timestep sampler Ï€lâ¢nâ¢(t;m,s)subscriptğœ‹ğ‘™ğ‘›ğ‘¡ğ‘šğ‘ \\pi_{ln}(t;m,s)italic_Ï€ start_POSTSUBSCRIPT italic_l italic_n end_POSTSUBSCRIPT ( italic_t ; italic_m , italic_s ), and evaluation is performed on 10,000 data points, with 1,000 timesteps sampled per point.\n\nLikelihood is our secondary metric. The likelihood over the dataset distribution Pğ’Ÿsubscriptğ‘ƒğ’ŸP_{\\mathcal{D}}italic_P start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT given model parameters Î¸ğœƒ\\thetaitalic_Î¸ is represented as ğ”¼xâˆ¼Pğ’Ÿâ¢[pÎ¸â¢(x)]subscriptğ”¼similar-toğ‘¥subscriptğ‘ƒğ’Ÿdelimited-[]subscriptğ‘ğœƒğ‘¥\\mathbb{E}_{x\\sim P_{\\mathcal{D}}}[p_{\\theta}(x)]blackboard_E start_POSTSUBSCRIPT italic_x âˆ¼ italic_P start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_p start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_x ) ], which can be challenging to compute directly. In this paper, we measure likelihood using two different methods. The first method is based on the Variational Autoencoder (VAE) framework (Kingma etÂ al., 2021; Song etÂ al., 2021; Vahdat etÂ al., 2021), which approximates the lower bound of log-likelihood using the Variational Lower Bound (VLB). Since the VAE component in our experiments is fixed to Stable Diffusion 1.5, terms related to the VAE remain constant and are ignored in our computation, as they do not affect the scaling behavior. The second method uses Neural Ordinary Differential Equations (ODEs) (Chen etÂ al., 2018; Grathwohl etÂ al., 2018), enabling the computation of exact likelihood through reverse-time sampling.\n\nVariational Lower Bound (VLB)\n\nWe estimate the VLB following the approach in Kingma etÂ al. (2021), where it is treated as a reweighted version of the validation loss. The process starts by converting the estimated velocity into a corresponding estimate of x0subscriptğ‘¥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, after which the loss is computed based on the difference between the estimated x0subscriptğ‘¥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and the original sample. To obtain the VLB, this loss is further reweighted with the weighting coefficient being the derivative of signal-to-noise ratio of noisy samples with respect to time tğ‘¡titalic_t, i.e., SNRâ€²â¢(t)superscriptSNRâ€²ğ‘¡\\text{SNR}^{{}^{\\prime}}(t)SNR start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT â€² end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT ( italic_t ). More details can be found in Appendix D. All models are evaluated on the COCO 2014 validation set using 10,000 data pairs. For each data point, 1,000 timesteps are sampled to ensure accurate estimations of the VLB.\n\nExact Likelihood\n\nThe exact likelihood is computed using reverse-time sampling, where a clean sample is transformed into Gaussian noise. The accumulated likelihood transition is calculated using the instantaneous change of variables theorem (Chen etÂ al., 2018):\n\nlogâ¡pÎ¸â¢(x)=logâ¡pÎ¸â¢(xT)âˆ’âˆ«0Tâˆ‡â‹…fÎ¸â¢(xt,t)â¢ğ‘‘t,subscriptğ‘ğœƒğ‘¥subscriptğ‘ğœƒsubscriptğ‘¥ğ‘‡superscriptsubscript0ğ‘‡â‹…âˆ‡subscriptğ‘“ğœƒsubscriptğ‘¥ğ‘¡ğ‘¡differential-dğ‘¡\\log p_{\\theta}(x)=\\log p_{\\theta}(x_{T})-\\int_{0}^{T}\\nabla\\cdot f_{\\theta}(x% _{t},t)\\,dt,roman_log italic_p start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_x ) = roman_log italic_p start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) - âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT âˆ‡ â‹… italic_f start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) italic_d italic_t ,\n\nwhere fÎ¸â¢(xt,t)subscriptğ‘“ğœƒsubscriptğ‘¥ğ‘¡ğ‘¡f_{\\theta}(x_{t},t)italic_f start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) represents the modelâ€™s output, and logâ¡pÎ¸â¢(xT)subscriptğ‘ğœƒsubscriptğ‘¥ğ‘‡\\log p_{\\theta}(x_{T})roman_log italic_p start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) is the log density of the final Gaussian noise. The reverse process evolves tğ‘¡titalic_t from 0 (data points) to Tğ‘‡Titalic_T (noise). The reverse sampling is performed over 500 steps using the Euler method.\n\nFollowing model training, we evaluate models on the validation set to assess their compute-optimal performance. As illustrated in Fig.Â 2, all four metrics (training loss, validation loss, offset VLB, and offset exact likelihood ) exhibit similar trends and shapes as the training budget increases, showing their utility in observing scaling laws. These consistent patterns suggest that any of these metrics can be effectively used to monitor scaling behavior. However, to simplify our experimental workflow, we prioritize training loss as the primary metric. Training loss can be observed directly during the training process, without the need for additional evaluation steps, making it a more practical choice for tracking scaling laws in real-time.\n\n\n3.2 Scaling Laws in Training Diffusion Transformers\n\nScaling Laws\n\nIn this section, we investigate the scaling laws governing diffusion transformers, which describe the relationships between several key quantities: the objective function, model parameters, tokens, and compute. The objective measures discrepancy between the data and modelâ€™s predictions. The number of parameters Nğ‘Nitalic_N reflects modelâ€™s capacity, while tokens Dğ·Ditalic_D denote the total amount of data (in tokens) processed during training. Compute Cğ¶Citalic_C, typically measured in Floating Point Operations (FLOPs), quantifies the total computational resources consumed. In our experiments, the relationship between compute, tokens, and model size is formalized as C=6â¢Nâ¢Dğ¶6ğ‘ğ·C=6NDitalic_C = 6 italic_N italic_D, directly linking the number of parameters and tokens to the overall compute budget.\n\nBuilding on this, we hypothesize that power law equations can effectively capture the scaling relationships between these quantities. Specifically, we represent the optimal model size and token count as functions of the compute budget as follows:\n\nNoptâˆCaandDoptâˆCb,formulae-sequenceproportional-tosubscriptğ‘optsuperscriptğ¶ğ‘andproportional-tosubscriptğ·optsuperscriptğ¶ğ‘\\displaystyle   (4)\nN_{\\text{opt}}\\propto C^{a}\\quad\\text{and}\\quad\nD_{\\text{opt}}% \\propto C^{b},italic_N start_POSTSUBSCRIPT\nopt end_POSTSUBSCRIPT âˆ italic_C start_POSTSUPERSCRIPT\nitalic_a end_POSTSUPERSCRIPT and italic_D\nstart_POSTSUBSCRIPT opt end_POSTSUBSCRIPT âˆ italic_C\nstart_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT ,\n\nwhere Noptsubscriptğ‘optN_{\\text{opt}}italic_N start_POSTSUBSCRIPT opt end_POSTSUBSCRIPT and Doptsubscriptğ·optD_{\\text{opt}}italic_D start_POSTSUBSCRIPT opt end_POSTSUBSCRIPT denote the optimal number of parameters and tokens for a given compute budget Cğ¶Citalic_C, with ağ‘aitalic_a and bğ‘bitalic_b as scaling exponents that describe how these quantities grow with compute.\n\nTo empirically verify these scaling relationships, following Approach 2 in Hoffmann etÂ al. (2022), we plot the isoFLOP figure to explore scaling laws. We select compute budgets [1e17, 3e17, 6e17, 1e18, 3e18, 6e18]. We change the In-context Transformers from 2 layers to 15 layers. For all experiments, we use AdamW (Kingma, 2014; Loshchilov, 2017) as the default optimizer. Except for the 1B parameter prediction experiments, we apply a constant learning rate of 1e-4. In the 1B parameter experiments, we train the model with a learning rate of 1e-4 for 80% of the total iterations, then decay it to 3.16e-5 for the next 10%, and finally to 1e-5 for the last 10%. This learning rate decay is crucial for larger models, leading to more accurate predictions. Following Wortsman etÂ al. (2023); Team (2024); Molybog etÂ al. (2023), we use a weight decay of 0.01, an epsilon value of 1e-15, and betas [0.9, 0.95]. For all experiments, we employ a batch size of 1024 and apply gradient clipping with a threshold of 1.0. To enable classifier-free guidance (Ho & Salimans, 2022), we randomly drop the label with a probability of 0.1. During training, we use mixed precision with the BF16 data format.\n\nFor each budget, we fit a parabola to the resulting performance curve, as illustrated in Fig.Â 1, to identify the optimal loss, model size, and data allocation (highlighted by the purple dots). By collecting the optimal values from different budgets and fitting them to a power law, we establish relationships between the optimal loss, model size, data, and compute budgets.\n\nAs shown in Fig.Â 1, except for the 1e17 budget, the parabolic fits align closely with the empirical results. This analysis confirms that the optimal number of parameters and tokens scale with the compute budget according to the following expressions:\n\nNoptsubscriptğ‘opt\\displaystyle N_{\\text{opt}}italic_N   =0.0009â‹…C0.5681,absentâ‹…0.0009superscriptğ¶0.5681\\displaystyle=0.0009\\cdot         (5)\nstart_POSTSUBSCRIPT opt end_POSTSUBSCRIPT                C^{0.5681},= 0.0009 â‹… italic_C start_POSTSUPERSCRIPT 0.5681\n                                                         end_POSTSUPERSCRIPT ,\nDoptsubscriptğ·opt\\displaystyle D_{\\text{opt}}italic_D   =186.8535â‹…C0.4319.absentâ‹…186.8535superscriptğ¶0.4319\\displaystyle=186.8535\\cdot   (6)\nstart_POSTSUBSCRIPT opt end_POSTSUBSCRIPT                C^{0.4319}.= 186.8535 â‹… italic_C start_POSTSUPERSCRIPT\n                                                         0.4319 end_POSTSUPERSCRIPT .\n\nIn this way, we establish the relationship between compute budget and model/data size. And from the fitted scaling curves, we observe that the ratio between the scaling exponent for data and the scaling exponent for model size is 0.4319/0.56810.43190.56810.4319/0.56810.4319 / 0.5681. This indicates that, under our specific settings, both the model and data sizes need to increase in tandem as the training budget increases. However, the model size grows at a slightly faster rate compared to the data size, as reflected by the proportional relationship between the two exponents.\n\nAdditionally, in Fig.Â 3, we fit the relationship between the compute budget and loss, which follows the equation:\n\nL=2.3943â‹…Câˆ’0.0273.ğ¿â‹…2.3943superscriptğ¶0.0273\\displaystyle   (7)\nL=2.3943\\cdot C^{-0.0273}.italic_L = 2.3943 â‹… italic_C\nstart_POSTSUPERSCRIPT - 0.0273 end_POSTSUPERSCRIPT .\n\nTo validate the accuracy of these fitted curves, we calculate the optimal model size (958.3M parameters) and token count for a compute budget of 1.5e21. A model is then trained with these specifications to compare its training loss with the predicted value. As demonstrated in Fig.Â 3, this modelâ€™s training loss closely matches the predicted loss, further confirming the validity of our scaling laws.\n\n\n3.3 Predicting Generation Performance\n\nIn generative tasks, relying solely on training loss to evaluate performance often falls short of capturing the perceptual quality of generated images. Training loss does not fully reflect how closely the generated images resemble real ones or how well they align with textual prompts in terms of content and style. To address this limitation, the FrÃ©chet Inception Distance (FID) is commonly used to evaluate image quality.\n\nFrÃ©chet Inception Distance (FID)\n\nFID measures the distance between the distributions of generated and real images in a feature space, with lower FID values indicating higher image quality. For our experiments, we utilize compute-optimal models at each training budget, generating samples using the Euler discretization method with 25 steps and a classifier-free guidance (CFG) scale of 10.0. Details on the impact of steps and CFG scale are provided in Appendix F.3. To compute FID, we follow the approach of Sauer etÂ al. (2021), using CLIP features instead of traditional Inception features. Specifically, we employ ViT-L/14 Dosovitskiy etÂ al. (2020) as the vision encoder to extract features from both generated and dataset images and compare their statistics.\n\nScaling laws for Performance Predictions\n\nOur analysis reveals that the relationship between FID and the training budget follows a clear power-law trend, as shown in Fig.Â 3 (left). The relationship is captured by the following equation:\n\nFID=2.2566Ã—106â‹…Câˆ’0.234,FIDâ‹…2.2566superscript106superscriptğ¶0.234\\displaystyle\\texttt{FID}=2.2566\\times   (8)\n10^{6}\\cdot C^{-0.234},FID = 2.2566 Ã— 10\nstart_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT â‹… italic_C\nstart_POSTSUPERSCRIPT - 0.234 end_POSTSUPERSCRIPT ,\n\nwhere Cğ¶Citalic_C is the training budget. The purple dots in the figure represent the FID scores of compute-optimal models at various budgets, and the orange line represents the fitted power-law curve. Notably, the prediction for FID at a large budget of 1.5e21 FLOPs (blue pentagon) is highly accurate, confirming the reliability of scaling laws in forecasting model performance even at larger scales. As the compute budget increases, FID values decrease consistently, demonstrating that scaling laws can effectively model and predict the quality of generated images as resources grow.\n\n\n3.4 Scaling Laws for Out-Of-Domain Data\n\nScaling laws remain valid even when models are tested on out-of-domain datasets. To demonstrate this, we conduct validation experiments on the COCO 2014 validation set (Lin etÂ al., 2014), using models that were trained on the Laion5B subset. In these experiments, we evaluate four key metrics: validation loss, Variational Lower Bound (VLB), exact likelihood, and FrÃ©chet Inception Distance (FID). Each metric is tested on 10,000 data points to examine the transferability of the scaling laws across datasets.\n\nThe results, as shown in Fig.Â 4, reveal two key observations:\n\n * â€¢\n   \n   Consistent Trends: Across all four metrics (validation loss, VLB, exact likelihood, and FID), the trends are consistent between the Laion5B subset and the COCO validation dataset. As the training budget increases, model performance improves in both cases, indicating that scaling laws generalize effectively across datasets, regardless of domain differences.\n\n * â€¢\n   \n   Vertical Offset: There is a clear vertical offset between the metrics for the Laion5B subset and the COCO validation dataset, with consistently higher metric values observed on the COCO dataset. This suggests that while scaling laws hold, the absolute performance is influenced by dataset-specific characteristics, such as complexity or distribution. For metrics like validation loss, VLB, and exact likelihood, this offset remains relatively constant across different training budgets. The gap between the FID values for the Laion5B subset and the COCO validation set widens as the training budget increases. However, the relationship between FID and budget on the COCO validation set still follows a power-law trend. This suggests that, even on out-of-domain data, the FID-budget relationship can be reliably modeled using a power law, allowing us to predict the modelâ€™s FID for a given budget.\n\nIn summary, these results demonstrate that scaling laws are robust and can be applied effectively to out-of-domain datasets, maintaining consistent trends while accounting for dataset-specific performance differences. Despite the vertical offset in absolute performance, particularly in metrics like FID, the power-law relationships remain intact, allowing for reliable predictions of model performance under varying budgets. These findings highlight the potential of scaling laws as a versatile tool for understanding model behavior across datasets. The ability to project model efficiency and performance onto new data domains underscores the broader applicability of scaling laws in real-world scenarios.\n\n\n4 Scaling laws as a predictable benchmark\n\nScaling laws offer a robust framework for evaluating the design quality of both models and datasets. Previous work such as Dubey etÂ al. (2024); Bi etÂ al. (2024) have all explored the scaling laws in data mix and quality. By modifying either the model architecture or the data pipeline, isoFLOP curves can be generated at smaller compute budgets to assess the impact of these changes. Specifically, after making adjustments to the model or dataset, experiments can be conducted across a range of smaller compute budgets, and the relationship between compute and metrics such as loss, parameter count, or token count can be fitted. The effectiveness of these modifications can then be evaluated by analyzing the exponents derived from the power-law fits.\n\nOur evaluation follows three key principles:\n\n * â€¢\n   \n   Model improvements: With a fixed dataset, a more efficient model will exhibit a lower model scaling exponent and a higher data scaling exponent. This suggests that the model can more effectively utilize the available data, allowing for a greater focus on increasing the dataset size with additional compute resources.\n\n * â€¢\n   \n   Data improvements: When the model is fixed, a higher-quality dataset will result in a lower data scaling exponent and a higher model scaling exponent. This implies that a better dataset enables the model to scale more efficiently, yielding superior results with fewer resources.\n\n * â€¢\n   \n   Loss improvements: Across both model and data modifications, an improved training pipeline is reflected in a smaller loss scaling exponent relative to compute. This indicates that the model achieves better performance with less compute, signaling overall gains in training efficiency.\n\nThe scaling trends for both models, In-Context Transformers and Cross-Attention Transformers, are clearly illustrated in Fig.Â 5. The loss scaling curves demonstrate that as the compute budget increases, the performance of both models improves, but at different rates. The Cross-Attention Transformer shows a steeper decline in loss compared to the In-Context Transformer, indicating that it achieves better performance with the same amount of compute.\n\nThe fitted scaling curves, as summarized in Tab.Â 1, support this observation. The Cross-Attention Transformer exhibits a larger model exponent, meaning that as compute budgets increase, more resources should be allocated toward scaling the dataset. Additionally, the smaller loss exponent of the Cross-Attention Transformer suggests a more rapid decline in loss, indicating that this model achieves superior performance compared to the In-Context Transformer. These findings align with the conclusions of Peebles & Xie (2023).\n\nThis example illustrates how scaling laws can serve as a reliable and predictable benchmark for evaluating the effectiveness of both model architectures and datasets. By analyzing the scaling exponents, we can draw meaningful conclusions about the potential and efficiency of different design choices in model and data pipelines.\n\n\n5 Discussion\n\nLimitations\n\nWhile our work demonstrates the presence of scaling laws in Diffusion Transformers (DiT), several limitations must be acknowledged. First, we used fixed hyperparameters, such as learning rate and batch size, across all experiments. A more comprehensive investigation into how these parameters should be adjusted as budgets scale may lead to more precise predictions. Second, our study focuses solely on text-to-image generation, leaving the scalability of Diffusion Transformers with other data modalities, such as video, unexplored. Finally, we limited our analysis to In-Context Transformers and Cross-Attention Transformers, leaving the evaluation of additional model variants to future research.\n\nConclusion\n\nIn this work, we explored the scaling laws of Diffusion Transformers (DiT) across a broad range of compute budgets, from 1e17 to 6e18 FLOPs, and confirmed the existence of a power-law relationship between pretraining loss and compute. This relationship enables precise predictions of optimal model size, data requirements, and model performance, even for large-scale budgets such as 1e21 FLOPs. Furthermore, we demonstrated the robustness of these scaling laws across different datasets, illustrating their generalizability beyond specific data distributions. In terms of generation performance, we showed that training budgets can be used to predict the visual quality of generated images, as measured by metrics like FID. Additionally, by testing both In-context Transformers and Cross-Attention Transformers, we validated the potential of scaling laws to serve as a predictable benchmark for evaluating and optimizing both model and data design, providing valuable guidance for future developments in text-to-image generation using DiT.\n\n\nReferences\n\n * Achiam etÂ al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, FlorenciaÂ Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, etÂ al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n * Albergo etÂ al. (2023) MichaelÂ S Albergo, NicholasÂ M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.\n * Bao etÂ al. (2023) Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.Â  22669â€“22679, 2023.\n * Bi etÂ al. (2024) Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, etÂ al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024.\n * Chen etÂ al. (2023) Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, etÂ al. Pixart-a: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023.\n * Chen etÂ al. (2018) RickyÂ TQ Chen, Yulia Rubanova, Jesse Bettencourt, and DavidÂ K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.\n * Dehghani etÂ al. (2023) Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, AndreasÂ Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, etÂ al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pp.Â  7480â€“7512. PMLR, 2023.\n * Devlin (2018) Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n * Dhariwal & Nichol (2021) Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780â€“8794, 2021.\n * Dinh etÂ al. (2016) Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016.\n * Dosovitskiy etÂ al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n * Dubey etÂ al. (2024) Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, etÂ al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n * Esser etÂ al. (2024) Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, etÂ al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024.\n * Grathwohl etÂ al. (2018) Will Grathwohl, RickyÂ TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord: Free-form continuous dynamics for scalable reversible generative models. arXiv preprint arXiv:1810.01367, 2018.\n * He etÂ al. (2022) Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.Â  16000â€“16009, 2022.\n * Henighan etÂ al. (2020) Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, TomÂ B Brown, Prafulla Dhariwal, Scott Gray, etÂ al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020.\n * Hestness etÂ al. (2017) Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, MdÂ MostofaÂ Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.\n * Ho & Salimans (2022) Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\n * Ho etÂ al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840â€“6851, 2020.\n * Hoffmann etÂ al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego deÂ Las Casas, LisaÂ Anne Hendricks, Johannes Welbl, Aidan Clark, etÂ al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n * Kaplan etÂ al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, TomÂ B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\n * Kingma etÂ al. (2021) Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:21696â€“21707, 2021.\n * Kingma (2013) DiederikÂ P Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\n * Kingma (2014) DiederikÂ P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n * Kingma & Dhariwal (2018) DurkÂ P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems, 31, 2018.\n * Li etÂ al. (2024) Hao Li, Yang Zou, Ying Wang, Orchid Majumder, Yusheng Xie, RÂ Manmatha, Ashwin Swaminathan, Zhuowen Tu, Stefano Ermon, and Stefano Soatto. On the scalability of diffusion-based text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.Â  9400â€“9409, 2024.\n * Lin etÂ al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and CÂ Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Visionâ€“ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp.Â  740â€“755. Springer, 2014.\n * Lipman etÂ al. (2022) Yaron Lipman, RickyÂ TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.\n * Liu etÂ al. (2024) Haotian Liu, Chunyuan Li, Yuheng Li, and YongÂ Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.Â  26296â€“26306, 2024.\n * Liu etÂ al. (2022) Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.\n * Loshchilov (2017) IÂ Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n * Lu etÂ al. (2024) Zeyu Lu, Zidong Wang, DiÂ Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, and Lei Bai. Fit: Flexible vision transformer for diffusion model. arXiv preprint arXiv:2402.12376, 2024.\n * Ma etÂ al. (2024) Nanye Ma, Mark Goldstein, MichaelÂ S Albergo, NicholasÂ M Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024.\n * Mei etÂ al. (2024) Kangfu Mei, Zhengzhong Tu, Mauricio Delbracio, Hossein Talebi, VishalÂ M Patel, and Peyman Milanfar. Bigger is not always better: Scaling properties of latent diffusion models. arXiv preprint arXiv:2404.01367, 2024.\n * Molybog etÂ al. (2023) Igor Molybog, Peter Albert, Moya Chen, Zachary DeVito, David Esiobu, Naman Goyal, PunitÂ Singh Koura, Sharan Narang, Andrew Poulton, Ruan Silva, etÂ al. A theory on adam instability in large-scale machine learning. arXiv preprint arXiv:2304.09871, 2023.\n * Peebles & Xie (2023) William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.Â  4195â€“4205, 2023.\n * Perez etÂ al. (2018) Ethan Perez, Florian Strub, Harm DeÂ Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, volumeÂ 32, 2018.\n * Radford (2018) Alec Radford. Improving language understanding by generative pre-training. 2018.\n * Radford etÂ al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, etÂ al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n * Ramesh etÂ al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pp.Â  8821â€“8831. Pmlr, 2021.\n * Rombach etÂ al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.Â  10684â€“10695, 2022.\n * Sauer etÂ al. (2021) Axel Sauer, Kashyap Chitta, Jens MÃ¼ller, and Andreas Geiger. Projected gans converge faster. Advances in Neural Information Processing Systems, 34:17480â€“17492, 2021.\n * Schuhmann etÂ al. (2022) Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, etÂ al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278â€“25294, 2022.\n * Sohl-Dickstein etÂ al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp.Â  2256â€“2265. PMLR, 2015.\n * Song & Ermon (2019) Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.\n * Song etÂ al. (2020) Yang Song, Jascha Sohl-Dickstein, DiederikÂ P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.\n * Song etÂ al. (2021) Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34:1415â€“1428, 2021.\n * Sun etÂ al. (2024) Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, YiÂ Wang, etÂ al. Journeydb: A benchmark for generative image understanding. Advances in Neural Information Processing Systems, 36, 2024.\n * Team (2024) Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024.\n * Vahdat etÂ al. (2021) Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. Advances in neural information processing systems, 34:11287â€“11302, 2021.\n * Vaswani (2017) AÂ Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017.\n * Wortsman etÂ al. (2023) Mitchell Wortsman, PeterÂ J Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, JohnÂ D Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, etÂ al. Small-scale proxies for large-scale transformer training instabilities. arXiv preprint arXiv:2309.14322, 2023.\n * Zhang & Sennrich (2019) Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019.\n * Zheng etÂ al. (2023) Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023.\n\n\nAppendix\n\nThis appendix is organized as follows:\n\n * â€¢\n   \n   In Section A, we list an overview of the notation used in the paper.\n\n * â€¢\n   \n   In Section B, we provide extended related work.\n\n * â€¢\n   \n   In Section C, we provide experimental details.\n\n * â€¢\n   \n   In Section D, we provide the derivation of the likelihood used in this paper.\n\n * â€¢\n   \n   In Section E, we provide the details of scaling FLOPs counting based on our models.\n\n * â€¢\n   \n   In Section F, we demonstrate ablative studies during the experiments.\n\n\nAppendix A Notation\n\nTab.Â 2 provides an overview of the notation used in this paper.\n\n\nAppendix B Extended Related Work\n\nDiffusion Models\n\nDiffusion models have gained significant attention due to their effectiveness in generative modeling, starting from discrete-time models (Sohl-Dickstein etÂ al., 2015; Ho etÂ al., 2020; Song & Ermon, 2019) to more recent continuous-time extensions (Song etÂ al., 2020). The core idea of diffusion models is to learn a sequence of noise-adding and denoising steps. In the forward process, noise is gradually added to the data, pushing it toward a Gaussian distribution, and in the reverse process, the model learns to iteratively denoise, recovering samples from the noise. Continuous-time variants (Song etÂ al., 2020) further generalize this framework using stochastic differential equations (SDEs), allowing for smoother control over the diffusion process. These methods leverage neural network architectures to model the score function and offer flexibility and better convergence properties compared to discrete versions. Diffusion models have shown remarkable success in various applications. For instance, ADM (Dhariwal & Nichol, 2021) outperforms GAN on ImageNet. Following this success, diffusion models have been extended to more complex tasks such as text-to-image generation. Notably, models like Stable Diffusion (Rombach etÂ al., 2022) and DALLE (Ramesh etÂ al., 2021) have demonstrated the ability to generate highly realistic and creative images from textual descriptions, representing a significant leap in the capabilities of generative models across various domains.\n\nNormalizing Flows\n\nNormalizing flows has been a popular generative modeling approach due to their ability to compute exact likelihoods while providing flexible and invertible transformations. Early works like GLOW (Kingma & Dhariwal, 2018) and RealNVP (Dinh etÂ al., 2016) introduced powerful architectures that allowed for efficient sampling and likelihood estimation. However, these models were limited by the necessity of designing specific bijective transformations, which constrained their expressiveness. To address these limitations, Neural ODE (Chen etÂ al., 2018) and FFJORD (Grathwohl etÂ al., 2018) extended normalizing flows to the continuous domain using differential equations. These continuous normalizing flows (CNFs) allowed for more flexible transformations by parameterizing them through neural networks and solving ODEs. By modeling the evolution of the probability density continuously, these methods achieved a higher level of expressiveness and adaptability compared to their discrete counterparts. Recent work has begun to bridge the gap between continuous normalizing flows and diffusion models. For instance, ScoreSDE (Song etÂ al., 2020) demonstrated how the connection between diffusion models and neural ODEs can be leveraged, allowing both exact likelihood computation and flexible generative processes. More recent models like Flow Matching (Lipman etÂ al., 2022) and Rectified Flow (Liu etÂ al., 2022) further combined the strengths of diffusion and flow-based models, enabling efficient training via diffusion processes while maintaining the ability to compute exact likelihoods for generated samples. In this paper, we build upon the formulation introduced by rectified flow and Flow Matching. By leveraging the training approach of diffusion models, we benefit from their generative performance, while retaining the capability to compute likelihoods.\n\nLikelihood Estimation\n\nLikelihood estimation in diffusion models can be approached from two primary perspectives: treating diffusion models as variational autoencoders (VAEs) or as normalizing flows. From the VAE perspective, diffusion models can be interpreted as models where we aim to optimize a variational lower bound (VLB) on the data likelihood (Kingma, 2013). The variational lower bound decomposes the data likelihood into a reconstruction term and a regularization term, where the latter measures the divergence between the approximate posterior and the prior. In diffusion models, this framework allows us to approximate the true posterior using a series of gradually noised latent variables. Recent works (Ho etÂ al., 2020; Kingma etÂ al., 2021; Song etÂ al., 2021) have derived tighter bounds for diffusion models, enabling more accurate likelihood estimation by optimizing this variational objective. Alternatively, diffusion models can be viewed as a form of normalizing flows, particularly in the context of continuous-time formulations. Using neural ODEs (Chen etÂ al., 2018), diffusion models can be trained to learn exact likelihoods by modeling the continuous reverse process as an ODE. By solving this reverse-time differential equation, one can directly compute the change in log-likelihood through the flow of probability densities (Grathwohl etÂ al., 2018). This approach provides a method for exact likelihood computation, bridging the gap between diffusion models and normalizing flows, and offering a more precise estimate of the likelihood for generative modeling.\n\n\nAppendix C Experimental Details\n\n\nC.1 Data\n\nWe primarily utilized three datasets in our work. Several ablation studies on formulation and model design were conducted using JourneyDB (Sun etÂ al., 2024). Additionally, we curated a subset of 108 million image-text pairs from the Laion-Aesthetic dataset, applying a threshold of 5 for the aesthetic score. After collecting the data, we re-captioned all images using LLAVA 1.5 (Liu etÂ al., 2024), specifically employing the LLaVA-Lightning-MPT-7B-preview model for this task. We then split the data into training and validation sets with a ratio of 100:1. Our third dataset is COCO (Lin etÂ al., 2014), where we used the 2014 validation set to test scaling laws on an out-of-domain dataset.\n\n\nC.2 Model Design\n\nIn this paper, we evaluate two distinct model architectures. For the PixArt model, we follow the original design presented in Chen etÂ al. (2023). The In-Context Transformers are based on the In-Context block described in Peebles & Xie (2023). To facilitate large-scale model training, we employ QK-Norm (Dehghani etÂ al., 2023) and RMSNorm (Zhang & Sennrich, 2019). The patch size is set to 2. Although previous work (Kaplan etÂ al., 2020) suggests that the aspect ratio (width/depth) of Transformers does not significantly impact scaling laws, it is crucial to maintain a consistent ratio when fitting models to scaling laws. To demonstrate this, we train a series of models under a fixed computational budget, selecting models of various sizes and aspect ratios (32 and 64). We then plot the relationship between the number of parameters and loss. As illustrated in Fig.Â 6, mixing models with aspect ratios of 64 and 32 obscures the overall trend. To address this issue, we maintain the aspect ratio at 64 throughout.\n\n\nAppendix D Derivation of the likelihood\n\nIn this section, we provide a derivation of the likelihood estimation in our paper. In this paper, we use two ways to compute the likelihood. The first method is estimating the VLB (variational lower bound). Following Kingma etÂ al. (2021); Vahdat etÂ al. (2021), we derive a VLB in latent space. However, we cannot compute the entropy terms in the VAE. So our surrogate metric differs from the true VLB up to a constant factor.\n\nVAE\n\nThe latent diffusion model (Vahdat etÂ al., 2021; Rombach etÂ al., 2022) consists of two components: a variational autoencoder (VAE) that encodes images into a latent space and decodes latents back into images, and a continuous diffusion model that operates in the latent space. To train the latent diffusion model, we optimize the variational encoder qÏ•subscriptğ‘italic-Ï•q_{\\phi}italic_q start_POSTSUBSCRIPT italic_Ï• end_POSTSUBSCRIPT, the decoder pÏˆsubscriptğ‘ğœ“p_{\\psi}italic_p start_POSTSUBSCRIPT italic_Ïˆ end_POSTSUBSCRIPT, and the diffusion model pÎ¸subscriptğ‘ğœƒp_{\\theta}italic_p start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT. Following Ho etÂ al. (2020), the models are trained by minimizing the variational upper bound on the negative log-likelihood logâ¡Pâ¢(x)ğ‘ƒğ‘¥\\log P(x)roman_log italic_P ( italic_x ):\n\nâ„’Î¸,Ï•,Ïˆâ¢(x)subscriptâ„’ğœƒitalic-Ï•ğœ“ğ‘¥\\displaystyle\\mathcal{L}_{\\theta,\\phi,\\psi}(x)caligraphic_L                                                                                                                                                                                                                                                                                                                                                                       =ğ”¼qÏ•â¢(z0|x)[âˆ’logpÏˆ(x|z0)]+KL(qÏ•(z0|x)||pÎ¸(z0))\\displaystyle=\\mathbb{E}_{q_{\\phi}(z_{0}|x)}[-\\log\nstart_POSTSUBSCRIPT italic_Î¸ , italic_Ï• , italic_Ïˆ                                                                                                                                                                                                                                                                                                                                                                                                                  p_{\\psi}(x|z_{0})]+KL(q_{%\nend_POSTSUBSCRIPT ( italic_x )                                                                                                                                                                                                                                                                                                                                                                                                                                      \\phi}(z_{0}|x)||p_{\\theta}(z_{0}))= blackboard_E\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                    start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_Ï•\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                    end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT 0\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                    end_POSTSUBSCRIPT | italic_x ) end_POSTSUBSCRIPT [ -\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                    roman_log italic_p start_POSTSUBSCRIPT italic_Ïˆ\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                    end_POSTSUBSCRIPT ( italic_x | italic_z start_POSTSUBSCRIPT\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                    0 end_POSTSUBSCRIPT ) ] + italic_K italic_L ( italic_q\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                    start_POSTSUBSCRIPT italic_Ï• end_POSTSUBSCRIPT ( italic_z\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                    start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | italic_x ) | |\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                    italic_p start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT (\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                    italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) )\n=ğ”¼qÏ•â¢(z0|x)â¢[âˆ’logâ¡pÏˆâ¢(x|z0)]+ğ”¼qÏ•â¢(z0|x)â¢[logâ¡qÏ•â¢(z0|x)]+ğ”¼qÏ•â¢(z0|x)â¢[âˆ’logâ¡pÎ¸â¢(z0)].absentsubscriptğ”¼subscriptğ‘italic-Ï•conditionalsubscriptğ‘§0ğ‘¥delimited-[]subscriptğ‘ğœ“conditionalğ‘¥subscriptğ‘§0subscriptğ”¼subscriptğ‘italic-Ï•conditionalsubscriptğ‘§0ğ‘¥delimited-[]subscriptğ‘italic-Ï•conditionalsubscriptğ‘§0ğ‘¥subscriptğ”¼subscriptğ‘italic-Ï•conditionalsubscriptğ‘§0ğ‘¥delimited-[]subscriptğ‘ğœƒsubscriptğ‘§0\\displaystyle=\\mathbb{E}_{q_{\\phi}(z_{0}|x)}[-\\log\np_{\\psi}(x|z_{0})]+\\mathbb{% E}_{q_{\\phi}(z_{0}|x)}[\\log\nq_{\\phi}(z_{0}|x)]+\\mathbb{E}_{q_{\\phi}(z_{0}|x)}[% -\\log\np_{\\theta}(z_{0})].= blackboard_E start_POSTSUBSCRIPT\nitalic_q start_POSTSUBSCRIPT italic_Ï• end_POSTSUBSCRIPT (\nitalic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | italic_x\n) end_POSTSUBSCRIPT [ - roman_log italic_p\nstart_POSTSUBSCRIPT italic_Ïˆ end_POSTSUBSCRIPT ( italic_x |\nitalic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) ] +\nblackboard_E start_POSTSUBSCRIPT italic_q\nstart_POSTSUBSCRIPT italic_Ï• end_POSTSUBSCRIPT ( italic_z\nstart_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | italic_x )\nend_POSTSUBSCRIPT [ roman_log italic_q start_POSTSUBSCRIPT\nitalic_Ï• end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT 0\nend_POSTSUBSCRIPT | italic_x ) ] + blackboard_E\nstart_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_Ï•\nend_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT 0\nend_POSTSUBSCRIPT | italic_x ) end_POSTSUBSCRIPT [ -\nroman_log italic_p start_POSTSUBSCRIPT italic_Î¸\nend_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT 0\nend_POSTSUBSCRIPT ) ] .\n\nIn our implementation, we directly adopt the VAE from Stable Diffusion 1.5 and keep it fixed during training. As a result, the reconstruction term (first term) and the negative encoder entropy term (second term) remain constant across different models. In fact, the VAE in Stable Diffusion is trained following the VQGAN approach, which uses both Lâ¢1ğ¿1L1italic_L 1 loss and an additional discriminator for training. Therefore, we cannot effectively estimate the reconstruction term since the decoder distribution is not tractable. To simplify further, we omit the VAE encoding process altogether. Specifically, we skip both encoding and decoding through the VAE and treat the latents produced by the VAE as the dataset samples. Under this assumption, we estimate the offset VLB directly in the latent space.\n\nIn the latent space, we model the distribution of latent variables that can be decoded into images using the VAE decoder. We denote the samples in latent space as xğ‘¥xitalic_x, and the noisy latent at timestep tğ‘¡titalic_t as ztsubscriptğ‘§ğ‘¡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. The variational lower bound (VLB) in the latent space is given by Kingma etÂ al. (2021):\n\nâˆ’logp(x)â‰¤âˆ’VLB(x)=DKâ¢L(q(z1|x)||p(z1))+ğ”¼qâ¢(z0|x)[âˆ’logp(x|z0)]+â„’T(x),\\displaystyle-\\log p(x)\\leq-\\text{VLB}(x)=D_{KL}(q(z_{1}|x)||p(z_{1}))+\\mathbb% {E}_{q(z_{0}|x)}[-\\log p(x|z_{0})]+\\mathcal{L}_{T}(x),- roman_log italic_p ( italic_x ) â‰¤ - VLB ( italic_x ) = italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q ( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_x ) | | italic_p ( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) + blackboard_E start_POSTSUBSCRIPT italic_q ( italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | italic_x ) end_POSTSUBSCRIPT [ - roman_log italic_p ( italic_x | italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) ] + caligraphic_L start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( italic_x ) ,\n\nwhere the first two terms depend only on the noise schedule, and we treat these terms as irreducible losses since the noise schedule is fixed across all models. The third term is the KL divergence between each pair of the reverse process pâ¢(zt|zt+1)ğ‘conditionalsubscriptğ‘§ğ‘¡subscriptğ‘§ğ‘¡1p(z_{t}|z_{t+1})italic_p ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) and the forward process qâ¢(zt|x,zt+1)ğ‘conditionalsubscriptğ‘§ğ‘¡ğ‘¥subscriptğ‘§ğ‘¡1q(z_{t}|x,z_{t+1})italic_q ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x , italic_z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ):\n\nâ„’T(x)=âˆ‘i=1Tğ”¼qâ¢(ztâ¢(i)|x)[DKâ¢L(q||p)].\\displaystyle\\mathcal{L}_{T}(x)=\\sum_{i=1}^{T}\\mathbb{E}_{q(z_{t(i)}|x)}[D_{KL% }(q||p)].caligraphic_L start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( italic_x ) = âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q ( italic_z start_POSTSUBSCRIPT italic_t ( italic_i ) end_POSTSUBSCRIPT | italic_x ) end_POSTSUBSCRIPT [ italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q | | italic_p ) ] .\n\nSince we assume that the forward and reverse processes share the same variance and both pğ‘pitalic_p and qğ‘qitalic_q are Gaussian distributions, the KL terms reduce to weighted Lâ¢2ğ¿2L2italic_L 2 distances:\n\nâ„’Tâ¢(x)=12â¢ğ”¼Ïµâˆ¼ğ’©â¢(0,I)â¢[âˆ‘i=1T(Sâ¢Nâ¢Râ¢(s)âˆ’Sâ¢Nâ¢Râ¢(t))â¢â€–xâˆ’xÎ¸â¢(zt;t)â€–22],subscriptâ„’ğ‘‡ğ‘¥12subscriptğ”¼similar-toitalic-Ïµğ’©0Idelimited-[]superscriptsubscriptğ‘–1ğ‘‡ğ‘†ğ‘ğ‘…ğ‘ ğ‘†ğ‘ğ‘…ğ‘¡superscriptsubscriptnormğ‘¥subscriptğ‘¥ğœƒsubscriptğ‘§ğ‘¡ğ‘¡22\\displaystyle\\mathcal{L}_{T}(x)=\\frac{1}{2}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}% (0,\\emph{I})}\\left[\\sum_{i=1}^{T}(SNR(s)-SNR(t))\\|x-x_{\\theta}(z_{t};t)\\|_{2}^% {2}\\right],caligraphic_L start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG blackboard_E start_POSTSUBSCRIPT italic_Ïµ âˆ¼ caligraphic_N ( 0 , I ) end_POSTSUBSCRIPT [ âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ( italic_S italic_N italic_R ( italic_s ) - italic_S italic_N italic_R ( italic_t ) ) âˆ¥ italic_x - italic_x start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; italic_t ) âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ,\n\nwhere s=tâˆ’1ğ‘ ğ‘¡1s=t-1italic_s = italic_t - 1. In the limit as Tâ†’âˆâ†’ğ‘‡T\\to\\inftyitalic_T â†’ âˆ, the loss becomes:\n\nâ„’Tâ¢(x)=âˆ’12â¢ğ”¼Ïµâˆ¼ğ’©â¢(0,I)â¢âˆ«01Sâ¢Nâ¢Râ€²â¢(t)â¢â€–xâˆ’xÎ¸â¢(zt;t)â€–22â¢ğ‘‘t.subscriptâ„’ğ‘‡ğ‘¥12subscriptğ”¼similar-toitalic-Ïµğ’©0Isuperscriptsubscript01ğ‘†ğ‘superscriptğ‘…â€²ğ‘¡superscriptsubscriptnormğ‘¥subscriptğ‘¥ğœƒsubscriptğ‘§ğ‘¡ğ‘¡22differential-dğ‘¡\\displaystyle\\mathcal{L}_{T}(x)=-\\frac{1}{2}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N% }(0,\\emph{I})}\\int_{0}^{1}SNR^{\\prime}(t)\\|x-x_{\\theta}(z_{t};t)\\|_{2}^{2}\\,dt.caligraphic_L start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( italic_x ) = - divide start_ARG 1 end_ARG start_ARG 2 end_ARG blackboard_E start_POSTSUBSCRIPT italic_Ïµ âˆ¼ caligraphic_N ( 0 , I ) end_POSTSUBSCRIPT âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT italic_S italic_N italic_R start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ( italic_t ) âˆ¥ italic_x - italic_x start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; italic_t ) âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d italic_t .\n\nIn our case, we utilize the velocity ğ¯ğ¯\\mathbf{v}bold_v to predict the clean sample xğ‘¥xitalic_x and compute the VLB.\n\nNormalizing Flows\n\nAnother method to compute the likelihood in our diffusion model is by viewing the diffusion process as a type of normalizing flow. Specifically, we leverage the theoretical results from Neural ODEs, which allow us to connect continuous normalizing flows with the evolution of probability density over time. In Neural ODEs, the transformation of data through the flow can be described by the following differential equation for the state variable xtsubscriptğ‘¥ğ‘¡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT as a function of time:\n\ndâ¢xtdâ¢t=fÎ¸â¢(xt,t),ğ‘‘subscriptğ‘¥ğ‘¡ğ‘‘ğ‘¡subscriptğ‘“ğœƒsubscriptğ‘¥ğ‘¡ğ‘¡\\frac{dx_{t}}{dt}=f_{\\theta}(x_{t},t),divide start_ARG italic_d italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_d italic_t end_ARG = italic_f start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) ,\n\nwhere fÎ¸â¢(xt,t)subscriptğ‘“ğœƒsubscriptğ‘¥ğ‘¡ğ‘¡f_{\\theta}(x_{t},t)italic_f start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) represents the network that predicts the time-dependent vector field ğ¯ğ¯\\mathbf{v}bold_v. To compute the change in log-probability of the transformed data, the log-likelihood of the input data under the flow is given by:\n\ndâ¢logâ¡pâ¢(xt)dâ¢t=âˆ’Trâ¢(âˆ‚fâ¢(xt,t)âˆ‚xt),ğ‘‘ğ‘subscriptğ‘¥ğ‘¡ğ‘‘ğ‘¡Trğ‘“subscriptğ‘¥ğ‘¡ğ‘¡subscriptğ‘¥ğ‘¡\\frac{d\\log p(x_{t})}{dt}=-\\text{Tr}\\left(\\frac{\\partial f(x_{t},t)}{\\partial x% _{t}}\\right),divide start_ARG italic_d roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG start_ARG italic_d italic_t end_ARG = - Tr ( divide start_ARG âˆ‚ italic_f ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) end_ARG start_ARG âˆ‚ italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ) ,\n\nwhere Tr represents the trace of the Jacobian matrix of fÎ¸â¢(xt,t)subscriptğ‘“ğœƒsubscriptğ‘¥ğ‘¡ğ‘¡f_{\\theta}(x_{t},t)italic_f start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ). This equation describes how the log-density evolves as the data is pushed forward through the flow. To compute the likelihood, we integrate the following expression over the trajectory from the initial state t0subscriptğ‘¡0t_{0}italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT to the terminal state t1subscriptğ‘¡1t_{1}italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT:\n\nlogâ¡pâ¢(xt1)=logâ¡pâ¢(xt0)âˆ’âˆ«t0t1Trâ¢(âˆ‚fÎ¸â¢(xt,t)âˆ‚xt)â¢ğ‘‘t.ğ‘subscriptğ‘¥subscriptğ‘¡1ğ‘subscriptğ‘¥subscriptğ‘¡0superscriptsubscriptsubscriptğ‘¡0subscriptğ‘¡1Trsubscriptğ‘“ğœƒsubscriptğ‘¥ğ‘¡ğ‘¡subscriptğ‘¥ğ‘¡differential-dğ‘¡\\log p(x_{t_{1}})=\\log p(x_{t_{0}})-\\int_{t_{0}}^{t_{1}}\\text{Tr}\\left(\\frac{% \\partial f_{\\theta}(x_{t},t)}{\\partial x_{t}}\\right)dt.roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) = roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) - âˆ« start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT Tr ( divide start_ARG âˆ‚ italic_f start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) end_ARG start_ARG âˆ‚ italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ) italic_d italic_t .\n\nHere, logâ¡pâ¢(xt0)ğ‘subscriptğ‘¥subscriptğ‘¡0\\log p(x_{t_{0}})roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) represents the log-likelihood of the initial state (often modeled as a Gaussian), and the integral accounts for the change in probability density over time as the data evolves through the ODE. In our formulation, the network predicts the velocity ğ¯Î¸â¢(xt,t)=xtâ€²=Î±tâ€²â¢x0+Î²tâ€²â¢Ïµsubscriptğ¯ğœƒsubscriptğ‘¥ğ‘¡ğ‘¡superscriptsubscriptğ‘¥ğ‘¡â€²subscriptsuperscriptğ›¼â€²ğ‘¡subscriptğ‘¥0subscriptsuperscriptğ›½â€²ğ‘¡italic-Ïµ\\mathbf{v}_{\\theta}(x_{t},t)=x_{t}^{{}^{\\prime}}=\\alpha^{{}^{\\prime}}_{t}x_{0}% +\\beta^{{}^{\\prime}}_{t}\\epsilonbold_v start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) = italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT â€² end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT = italic_Î± start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT â€² end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_Î² start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT â€² end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_Ïµ, which corresponds to the derivative of xtsubscriptğ‘¥ğ‘¡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT with respect to time. Thus, we start with clean samples, estimate the velocity, perform an iterative reverse-time sampling, and convert the samples into Gaussian noise. We can then compute the prior likelihood of the noise easily and add it to the probability shift accumulated during reverse sampling. In our experiments, we set the steps of reverse sampling to 500 to obtain a rather accurate estimation.\n\n\nAppendix E Scaling FLOPs Counting\n\nIn this section, we provide a detailed explanation of our FLOPs scaling calculations. Several prior works have employed different methods for counting FLOPs. In Kaplan etÂ al. (2020), the authors exclude embedding matrices, bias terms, and sub-leading terms. Moreover, under their framework, the model dimension dmâ¢oâ¢dâ¢eâ¢lsubscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™d_{model}italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT is much larger than the context length lcâ¢tâ¢xsubscriptğ‘™ğ‘ğ‘¡ğ‘¥l_{ctx}italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT, allowing them to disregard context-dependent terms. Consequently, the FLOPs count Nğ‘Nitalic_N for their model is given by:\n\nM=12Ã—dmâ¢oâ¢dâ¢eâ¢lÃ—nlâ¢aâ¢yâ¢eâ¢rÃ—(2â¢daâ¢tâ¢tâ¢n+dfâ¢f),ğ‘€12subscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™subscriptğ‘›ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿ2subscriptğ‘‘ğ‘ğ‘¡ğ‘¡ğ‘›subscriptğ‘‘ğ‘“ğ‘“\\displaystyle   (9)\nM=12\\times d_{model}\\times\nn_{layer}\\times(2d_{attn}+d_{ff}),italic_M = 12 Ã— italic_d\nstart_POSTSUBSCRIPT italic_m italic_o italic_d italic_e\nitalic_l end_POSTSUBSCRIPT Ã— italic_n start_POSTSUBSCRIPT\nitalic_l italic_a italic_y italic_e italic_r\nend_POSTSUBSCRIPT Ã— ( 2 italic_d start_POSTSUBSCRIPT\nitalic_a italic_t italic_t italic_n end_POSTSUBSCRIPT +\nitalic_d start_POSTSUBSCRIPT italic_f italic_f\nend_POSTSUBSCRIPT ) ,\n\nwhere dmâ¢oâ¢dâ¢eâ¢lsubscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™d_{model}italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT represents the model dimension, nlâ¢aâ¢yâ¢eâ¢rsubscriptğ‘›ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿn_{layer}italic_n start_POSTSUBSCRIPT italic_l italic_a italic_y italic_e italic_r end_POSTSUBSCRIPT denotes the depth of the model, daâ¢tâ¢tâ¢nsubscriptğ‘‘ğ‘ğ‘¡ğ‘¡ğ‘›d_{attn}italic_d start_POSTSUBSCRIPT italic_a italic_t italic_t italic_n end_POSTSUBSCRIPT refers to the attention dimension, and dfâ¢fsubscriptğ‘‘ğ‘“ğ‘“d_{ff}italic_d start_POSTSUBSCRIPT italic_f italic_f end_POSTSUBSCRIPT represents the feed-forward layer dimension.\n\nIn contrast, Hoffmann etÂ al. (2022) includes all training FLOPs, accounting for embedding matrices, attention mechanisms, dense blocks, logits, and context-dependent terms. Specifically, their FLOP computation includes:\n\n * â€¢\n   \n   Embedding: 2Ã—lcâ¢tâ¢xÃ—Nvâ¢oâ¢cÃ—dmâ¢oâ¢dâ¢eâ¢l2subscriptğ‘™ğ‘ğ‘¡ğ‘¥subscriptğ‘ğ‘£ğ‘œğ‘subscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™2\\times l_{ctx}\\times N_{voc}\\times d_{model}2 Ã— italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT Ã— italic_N start_POSTSUBSCRIPT italic_v italic_o italic_c end_POSTSUBSCRIPT Ã— italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT\n\n * â€¢\n   \n   Attention:\n   \n    * â€“\n      \n      QKV Mapping: 2Ã—3Ã—lcâ¢tâ¢xÃ—dmâ¢oâ¢dâ¢eâ¢lÃ—dmâ¢oâ¢dâ¢eâ¢l23subscriptğ‘™ğ‘ğ‘¡ğ‘¥subscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™subscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™2\\times 3\\times l_{ctx}\\times d_{model}\\times d_{model}2 Ã— 3 Ã— italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT Ã— italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT Ã— italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT\n   \n    * â€“\n      \n      QK: 2Ã—lcâ¢tâ¢xÃ—lcâ¢tâ¢xÃ—dmâ¢oâ¢dâ¢eâ¢l2subscriptğ‘™ğ‘ğ‘¡ğ‘¥subscriptğ‘™ğ‘ğ‘¡ğ‘¥subscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™2\\times l_{ctx}\\times l_{ctx}\\times d_{model}2 Ã— italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT Ã— italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT Ã— italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT\n   \n    * â€“\n      \n      Softmax: 3Ã—nhâ¢eâ¢aâ¢dÃ—lcâ¢tâ¢xÃ—lcâ¢tâ¢x3subscriptğ‘›â„ğ‘’ğ‘ğ‘‘subscriptğ‘™ğ‘ğ‘¡ğ‘¥subscriptğ‘™ğ‘ğ‘¡ğ‘¥3\\times n_{head}\\times l_{ctx}\\times l_{ctx}3 Ã— italic_n start_POSTSUBSCRIPT italic_h italic_e italic_a italic_d end_POSTSUBSCRIPT Ã— italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT Ã— italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT\n   \n    * â€“\n      \n      Mask: 2Ã—lcâ¢tâ¢xÃ—lcâ¢tâ¢xÃ—dmâ¢oâ¢dâ¢eâ¢l2subscriptğ‘™ğ‘ğ‘¡ğ‘¥subscriptğ‘™ğ‘ğ‘¡ğ‘¥subscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™2\\times l_{ctx}\\times l_{ctx}\\times d_{model}2 Ã— italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT Ã— italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT Ã— italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT\n   \n    * â€“\n      \n      Projection: 2Ã—lcâ¢tâ¢xÃ—dmâ¢oâ¢dâ¢eâ¢lÃ—dmâ¢oâ¢dâ¢eâ¢l2subscriptğ‘™ğ‘ğ‘¡ğ‘¥subscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™subscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™2\\times l_{ctx}\\times d_{model}\\times d_{model}2 Ã— italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT Ã— italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT Ã— italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT\n\n * â€¢\n   \n   Dense: 2Ã—lcâ¢tâ¢xÃ—(dmâ¢oâ¢dâ¢eâ¢lÃ—dfâ¢fÃ—2)2subscriptğ‘™ğ‘ğ‘¡ğ‘¥subscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™subscriptğ‘‘ğ‘“ğ‘“22\\times l_{ctx}\\times(d_{model}\\times d_{ff}\\times 2)2 Ã— italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT Ã— ( italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT Ã— italic_d start_POSTSUBSCRIPT italic_f italic_f end_POSTSUBSCRIPT Ã— 2 )\n\n * â€¢\n   \n   Logits: 2Ã—lcâ¢tâ¢xÃ—dmâ¢oâ¢dâ¢eâ¢lÃ—Nvâ¢oâ¢c2subscriptğ‘™ğ‘ğ‘¡ğ‘¥subscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™subscriptğ‘ğ‘£ğ‘œğ‘2\\times l_{ctx}\\times d_{model}\\times N_{voc}2 Ã— italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT Ã— italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT Ã— italic_N start_POSTSUBSCRIPT italic_v italic_o italic_c end_POSTSUBSCRIPT\n\nFurther details can be found in the Appendix F of Hoffmann etÂ al. (2022).\n\nIn Bi etÂ al. (2024), the authors omit the embedding computation but retain the context-dependent terms, which aligns with our approach. The parameter scaling calculation for vanilla Transformers in this paper follows the same format as theirs. We now present detailed scaling FLOPs calculations for the In-Context Transformers and Cross-Attn Transformers used in our experiments.\n\nAttention blocks are the primary components responsible for scaling in Transformer architectures. In line with previous studies, we only consider the attention blocks, excluding embedding matrices and sub-leading terms. Unlike large language models (LLMs), our model dimension is comparable to the context length, and therefore, we include context-dependent terms. In this section, we present FLOPs per sample rather than parameters, as different tokens participate in different parts of the cross-attention computation. Additionally, since our input length is fixed, the FLOPs per sample are straightforward to compute.\n\nIn-Context Transformers\n\nIn-Context Transformers process a joint embedding consisting of text, image, and time tokens, all of which undergo attention computation. Tab.Â 3 details the FLOPs calculations for a single attention layer.\n\nIn our experiments, we set dmâ¢oâ¢dâ¢eâ¢l=daâ¢tâ¢tâ¢nsubscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™subscriptğ‘‘ğ‘ğ‘¡ğ‘¡ğ‘›d_{model}=d_{attn}italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT italic_a italic_t italic_t italic_n end_POSTSUBSCRIPT, and lcâ¢tâ¢x=377subscriptğ‘™ğ‘ğ‘¡ğ‘¥377l_{ctx}=377italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT = 377, where lcâ¢tâ¢x=liâ¢mâ¢gâ¢(256)+ltâ¢eâ¢xâ¢tâ¢(120)+ltâ¢iâ¢mâ¢eâ¢(1)subscriptğ‘™ğ‘ğ‘¡ğ‘¥subscriptğ‘™ğ‘–ğ‘šğ‘”256subscriptğ‘™ğ‘¡ğ‘’ğ‘¥ğ‘¡120subscriptğ‘™ğ‘¡ğ‘–ğ‘šğ‘’1l_{ctx}=l_{img}(256)+l_{text}(120)+l_{time}(1)italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT = italic_l start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT ( 256 ) + italic_l start_POSTSUBSCRIPT italic_t italic_e italic_x italic_t end_POSTSUBSCRIPT ( 120 ) + italic_l start_POSTSUBSCRIPT italic_t italic_i italic_m italic_e end_POSTSUBSCRIPT ( 1 ). Thus, the simplified FLOPs-per-sample scaling equation Mğ‘€Mitalic_M is:\n\nM=72Ã—lcâ¢tâ¢xÃ—nlâ¢aâ¢yâ¢eâ¢rÃ—dmâ¢oâ¢dâ¢eâ¢l2+12Ã—nlâ¢aâ¢yâ¢eâ¢rÃ—lcâ¢tâ¢x2Ã—dmâ¢oâ¢dâ¢eâ¢lğ‘€72subscriptğ‘™ğ‘ğ‘¡ğ‘¥subscriptğ‘›ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿsubscriptsuperscriptğ‘‘2ğ‘šğ‘œğ‘‘ğ‘’ğ‘™12subscriptğ‘›ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿsuperscriptsubscriptğ‘™ğ‘ğ‘¡ğ‘¥2subscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™\\displaystyle   (10)\nM=72\\times l_{ctx}\\times n_{layer}\\times\nd^{2}_{model}+12\\times n% _{layer}\\times l_{ctx}^{2}\\times\nd_{model}italic_M = 72 Ã— italic_l start_POSTSUBSCRIPT\nitalic_c italic_t italic_x end_POSTSUBSCRIPT Ã— italic_n\nstart_POSTSUBSCRIPT italic_l italic_a italic_y italic_e\nitalic_r end_POSTSUBSCRIPT Ã— italic_d start_POSTSUPERSCRIPT\n2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_o\nitalic_d italic_e italic_l end_POSTSUBSCRIPT + 12 Ã— italic_n\nstart_POSTSUBSCRIPT italic_l italic_a italic_y italic_e\nitalic_r end_POSTSUBSCRIPT Ã— italic_l start_POSTSUBSCRIPT\nitalic_c italic_t italic_x end_POSTSUBSCRIPT\nstart_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT Ã— italic_d\nstart_POSTSUBSCRIPT italic_m italic_o italic_d italic_e\nitalic_l end_POSTSUBSCRIPT\n\nCross-Attn Transformers\n\nIn Cross-Attn Transformers, each attention block consists of self-attention and cross-attention mechanisms to integrate text information. The cross-attention uses image embeddings as the query and text embeddings as the key and value. The attention mask reflects the cross-modal similarity between image patches and text segments. As a result, the FLOPs calculation differs from that of models using joint text-image embeddings. Tab.Â 4 lists the FLOPs costs for each operation.\n\nBased on the experimental settings, we can simplify the FLOPs calculation as follows:\n\nM=84Ã—nlâ¢aâ¢yâ¢eâ¢rÃ—liâ¢mâ¢gÃ—dmâ¢oâ¢dâ¢eâ¢l2+12Ã—nlâ¢aâ¢yâ¢eâ¢rÃ—liâ¢mâ¢g2Ã—daâ¢tâ¢tâ¢nğ‘€84subscriptğ‘›ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿsubscriptğ‘™ğ‘–ğ‘šğ‘”superscriptsubscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™212subscriptğ‘›ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿsuperscriptsubscriptğ‘™ğ‘–ğ‘šğ‘”2subscriptğ‘‘ğ‘ğ‘¡ğ‘¡ğ‘›\\displaystyle\nM=84\\times n_{layer}\\times l_{img}\\times\nd_{model}^{2}+12\\times n% _{layer}\\times l_{img}^{2}\\times\nd_{attn}italic_M = 84 Ã— italic_n start_POSTSUBSCRIPT\nitalic_l italic_a italic_y italic_e italic_r\nend_POSTSUBSCRIPT Ã— italic_l start_POSTSUBSCRIPT italic_i\nitalic_m italic_g end_POSTSUBSCRIPT Ã— italic_d\nstart_POSTSUBSCRIPT italic_m italic_o italic_d italic_e\nitalic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2\nend_POSTSUPERSCRIPT + 12 Ã— italic_n start_POSTSUBSCRIPT\nitalic_l italic_a italic_y italic_e italic_r\nend_POSTSUBSCRIPT Ã— italic_l start_POSTSUBSCRIPT italic_i\nitalic_m italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2\nend_POSTSUPERSCRIPT Ã— italic_d start_POSTSUBSCRIPT italic_a\nitalic_t italic_t italic_n end_POSTSUBSCRIPT\n+12Ã—nlâ¢aâ¢yâ¢eâ¢rÃ—ltâ¢eâ¢xâ¢tÃ—dmâ¢oâ¢dâ¢eâ¢l2+12Ã—nlâ¢aâ¢yâ¢eâ¢rÃ—ltâ¢eâ¢xâ¢tÃ—liâ¢mâ¢gÃ—dmâ¢oâ¢dâ¢eâ¢l12subscriptğ‘›ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿsubscriptğ‘™ğ‘¡ğ‘’ğ‘¥ğ‘¡superscriptsubscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™212subscriptğ‘›ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿsubscriptğ‘™ğ‘¡ğ‘’ğ‘¥ğ‘¡subscriptğ‘™ğ‘–ğ‘šğ‘”subscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™\\displaystyle+12\\times   (11)\nn_{layer}\\times l_{text}\\times d_{model}^{2}+12\\times n%\n_{layer}\\times l_{text}\\times l_{img}\\times d_{model}+ 12 Ã—\nitalic_n start_POSTSUBSCRIPT italic_l italic_a italic_y\nitalic_e italic_r end_POSTSUBSCRIPT Ã— italic_l\nstart_POSTSUBSCRIPT italic_t italic_e italic_x italic_t\nend_POSTSUBSCRIPT Ã— italic_d start_POSTSUBSCRIPT italic_m\nitalic_o italic_d italic_e italic_l end_POSTSUBSCRIPT\nstart_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 12 Ã— italic_n\nstart_POSTSUBSCRIPT italic_l italic_a italic_y italic_e\nitalic_r end_POSTSUBSCRIPT Ã— italic_l start_POSTSUBSCRIPT\nitalic_t italic_e italic_x italic_t end_POSTSUBSCRIPT Ã—\nitalic_l start_POSTSUBSCRIPT italic_i italic_m italic_g\nend_POSTSUBSCRIPT Ã— italic_d start_POSTSUBSCRIPT italic_m\nitalic_o italic_d italic_e italic_l end_POSTSUBSCRIPT\n\nContext-Dependent Terms\n\nFrom the equations above, it is evident that some context-dependent terms, such as 12Ã—nlâ¢aâ¢yâ¢eâ¢rÃ—lcâ¢tâ¢x2Ã—dmâ¢oâ¢dâ¢eâ¢l12subscriptğ‘›ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿsuperscriptsubscriptğ‘™ğ‘ğ‘¡ğ‘¥2subscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™12\\times n_{layer}\\times l_{ctx}^{2}\\times d_{model}12 Ã— italic_n start_POSTSUBSCRIPT italic_l italic_a italic_y italic_e italic_r end_POSTSUBSCRIPT Ã— italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT Ã— italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT, cannot be omitted. In our experiments, the aspect ratio of Transformers (width/depth=64) is maintained across all model sizes. The context length lcâ¢tâ¢xsubscriptğ‘™ğ‘ğ‘¡ğ‘¥l_{ctx}italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT is 377 (image: 256, text: 120, time: 1), and dmâ¢oâ¢dâ¢eâ¢l=nlâ¢aâ¢yâ¢eâ¢rÃ—64subscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™subscriptğ‘›ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿ64d_{model}=n_{layer}\\times 64italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT = italic_n start_POSTSUBSCRIPT italic_l italic_a italic_y italic_e italic_r end_POSTSUBSCRIPT Ã— 64. Since lcâ¢tâ¢xsubscriptğ‘™ğ‘ğ‘¡ğ‘¥l_{ctx}italic_l start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT and dmâ¢oâ¢dâ¢eâ¢lsubscriptğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™d_{model}italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT are comparable, the context-dependent terms must be retained.\n\n\nAppendix F Ablations\n\n\nF.1 Diffusion Formulation\n\nIn diffusion models, various formulations for noise schedules, timestep schedules, and prediction objectives have been proposed. These three components are interdependent and require specific tuning to achieve optimal performance. In this paper, we explore several common formulations and conduct ablation studies to identify the best combination in our setting.\n\nBelow, we list the candidate formulations used in our ablation study.\n\nNoise Schedule\n\nDiscrete Diffusion Models\n\nDDPM\n\nDenoising Diffusion Probabilistic Models (DDPM) (Ho etÂ al., 2020) is a discrete-time diffusion model that generates noisy samples via the following formula:\n\nxt=Î±tâ¢x0+Î²tâ¢Ïµsubscriptğ‘¥ğ‘¡subscriptğ›¼ğ‘¡subscriptğ‘¥0subscriptğ›½ğ‘¡italic-Ïµx_{t}=\\alpha_{t}x_{0}+\\beta_{t}\\epsilonitalic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_Ïµ\n\nwhere Ïµitalic-Ïµ\\epsilonitalic_Ïµ is Gaussian noise, and Î±tsubscriptğ›¼ğ‘¡\\alpha_{t}italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and Î²tsubscriptğ›½ğ‘¡\\beta_{t}italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT satisfy Î±t2+Î²t2=1superscriptsubscriptğ›¼ğ‘¡2superscriptsubscriptğ›½ğ‘¡21\\alpha_{t}^{2}+\\beta_{t}^{2}=1italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1. Given a sequence of Ïƒtsubscriptğœğ‘¡\\sigma_{t}italic_Ïƒ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, the scaling factor can be defined as:\n\nÎ±t=âˆs=0t(1âˆ’Ïƒt)subscriptğ›¼ğ‘¡superscriptsubscriptproductğ‘ 0ğ‘¡1subscriptğœğ‘¡\\displaystyle\\alpha_{t}=\\sqrt{\\prod_{s=0}^{t}(1-\\sigma_{t})}italic_Î±   (12)\nstart_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = square-root\nstart_ARG âˆ start_POSTSUBSCRIPT italic_s = 0\nend_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t\nend_POSTSUPERSCRIPT ( 1 - italic_Ïƒ start_POSTSUBSCRIPT\nitalic_t end_POSTSUBSCRIPT ) end_ARG\n\nIn DDPM, Ïƒtsubscriptğœğ‘¡\\sigma_{t}italic_Ïƒ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT follows:\n\nÏƒt=Ïƒ0+tTâ¢(ÏƒTâˆ’Ïƒ0)subscriptğœğ‘¡subscriptğœ0ğ‘¡ğ‘‡subscriptğœğ‘‡subscriptğœ0\\displaystyle\\sigma_{t}=\\sigma_{0}+\\frac{t}{T}(\\sigma_{T}-\\sigma_{0})italic_Ïƒ   (13)\nstart_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_Ïƒ\nstart_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + divide start_ARG\nitalic_t end_ARG start_ARG italic_T end_ARG ( italic_Ïƒ\nstart_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT - italic_Ïƒ\nstart_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )\n\nLDM\n\nLatent Diffusion Models (LDM) (Rombach etÂ al., 2022), as used in Stable Diffusion, is a variant of the DDPM schedule. It is also a variance-preserving formulation, sharing the same structure as DDPM but employing a different noise schedule:\n\nÏƒt=(Ïƒ0+tTâ¢(ÏƒTâˆ’Ïƒ0))2subscriptğœğ‘¡superscriptsubscriptğœ0ğ‘¡ğ‘‡subscriptğœğ‘‡subscriptğœ02\\sigma_{t}=\\left(\\sqrt{\\sigma_{0}}+\\frac{t}{T}\\left(\\sqrt{\\sigma_{T}}-\\sqrt{% \\sigma_{0}}\\right)\\right)^{2}italic_Ïƒ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ( square-root start_ARG italic_Ïƒ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG + divide start_ARG italic_t end_ARG start_ARG italic_T end_ARG ( square-root start_ARG italic_Ïƒ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_ARG - square-root start_ARG italic_Ïƒ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\n\nContinuous Diffusion Models\n\nVP\n\nVariance Preserving (VP) diffusion (Song etÂ al., 2020) is the continuous counterpart of DDPM, where the noise is added while preserving variance across timesteps. The sampling process is given by:\n\nxt=eâˆ’12â¢âˆ«0tÏƒsâ¢ğ‘‘sâ¢x0+1âˆ’eâˆ’âˆ«0tÏƒsâ¢ğ‘‘sâ¢Ïµsubscriptğ‘¥ğ‘¡superscriptğ‘’12superscriptsubscript0ğ‘¡subscriptğœğ‘ differential-dğ‘ subscriptğ‘¥01superscriptğ‘’superscriptsubscript0ğ‘¡subscriptğœğ‘ differential-dğ‘ italic-Ïµx_{t}=e^{-\\frac{1}{2}\\int_{0}^{t}\\sigma_{s}ds}x_{0}+\\sqrt{1-e^{-\\int_{0}^{t}% \\sigma_{s}ds}}\\epsilonitalic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_e start_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_Ïƒ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT italic_d italic_s end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + square-root start_ARG 1 - italic_e start_POSTSUPERSCRIPT - âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_Ïƒ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT italic_d italic_s end_POSTSUPERSCRIPT end_ARG italic_Ïµ\n\nwhere tâˆˆ[0,1]ğ‘¡01t\\in[0,1]italic_t âˆˆ [ 0 , 1 ].\n\nRectified Flow\n\nRectified Flow (RF) (Liu etÂ al., 2022; Lipman etÂ al., 2022; Albergo etÂ al., 2023) is another continuous-time formulation, where a straight-line interpolation is defined between the initial sample x0subscriptğ‘¥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and the Gaussian noise Ïµitalic-Ïµ\\epsilonitalic_Ïµ. The process is described by:\n\nxt=(1âˆ’t)â¢x0+tâ¢Ïµsubscriptğ‘¥ğ‘¡1ğ‘¡subscriptğ‘¥0ğ‘¡italic-Ïµx_{t}=(1-t)x_{0}+t\\epsilonitalic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ( 1 - italic_t ) italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_t italic_Ïµ\n\nPrediction Type\n\nNoise Prediction (Ïµbold-italic-Ïµ\\bm{\\epsilon}bold_italic_Ïµ)\n\nThe network predicts the Gaussian noise Ïµâˆ¼ğ’©â¢(0,I)similar-toitalic-Ïµğ’©0ğ¼\\epsilon\\sim\\mathcal{N}(0,I)italic_Ïµ âˆ¼ caligraphic_N ( 0 , italic_I ) added to the samples during the diffusion process.\n\nVelocity Prediction (ğ¯ğ¯\\mathbf{v}bold_v)\n\nIn this formulation, the network predicts the velocity ğ¯â¢(xt,t)ğ¯subscriptğ‘¥ğ‘¡ğ‘¡\\mathbf{v}(x_{t},t)bold_v ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ), which is defined as the derivative of the noisy sample xtsubscriptğ‘¥ğ‘¡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT with respect to time. If the noisy sample xtsubscriptğ‘¥ğ‘¡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is defined by:\n\nxt=Î±tâ¢x0+Î²tâ¢Ïµsubscriptğ‘¥ğ‘¡subscriptğ›¼ğ‘¡subscriptğ‘¥0subscriptğ›½ğ‘¡italic-Ïµx_{t}=\\alpha_{t}x_{0}+\\beta_{t}\\epsilonitalic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_Ïµ\n\nthe velocity is given by:\n\nğ¯â¢(xt,t)=xtâ€²=Î±tâ€²â¢x0+Î²tâ€²â¢Ïµğ¯subscriptğ‘¥ğ‘¡ğ‘¡superscriptsubscriptğ‘¥ğ‘¡â€²superscriptsubscriptğ›¼ğ‘¡â€²subscriptğ‘¥0superscriptsubscriptğ›½ğ‘¡â€²italic-Ïµ\\mathbf{v}(x_{t},t)=x_{t}^{\\prime}=\\alpha_{t}^{\\prime}x_{0}+\\beta_{t}^{\\prime}\\epsilonbold_v ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) = italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT italic_Ïµ\n\nwhere Î±tâ€²superscriptsubscriptğ›¼ğ‘¡â€²\\alpha_{t}^{\\prime}italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT and Î²tâ€²superscriptsubscriptğ›½ğ‘¡â€²\\beta_{t}^{\\prime}italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT are the derivatives of Î±tsubscriptğ›¼ğ‘¡\\alpha_{t}italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and Î²tsubscriptğ›½ğ‘¡\\beta_{t}italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT with respect to timestep tğ‘¡titalic_t.\n\nScore Prediction (ğ¬ğ¬\\mathbf{s}bold_s)\n\nThe network predicts the score function ğ¬â¢(x,t)=âˆ‡logâ¡Pâ¢(x,t)ğ¬ğ‘¥ğ‘¡âˆ‡ğ‘ƒğ‘¥ğ‘¡\\mathbf{s}(x,t)=\\nabla\\log P(x,t)bold_s ( italic_x , italic_t ) = âˆ‡ roman_log italic_P ( italic_x , italic_t ), which is the gradient of the log-probability density function. The score can be derived as:\n\nğ¬â¢(x,t)=âˆ’ÏµÎ²tğ¬ğ‘¥ğ‘¡italic-Ïµsubscriptğ›½ğ‘¡\\mathbf{s}(x,t)=-\\frac{\\epsilon}{\\beta_{t}}bold_s ( italic_x , italic_t ) = - divide start_ARG italic_Ïµ end_ARG start_ARG italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG\n\nTimestep Sampling Schedule\n\nUniform Timestep Schedule\n\nIn this schedule, the timestep tğ‘¡titalic_t is uniformly sampled. For discrete-time diffusion models:\n\ntâˆ¼ğ’°â¢(0,1,2,â€¦,999)similar-toğ‘¡ğ’°012â€¦999t\\sim\\mathcal{U}(0,1,2,\\ldots,999)italic_t âˆ¼ caligraphic_U ( 0 , 1 , 2 , â€¦ , 999 )\n\nFor continuous-time diffusion models:\n\ntâˆ¼ğ’°â¢(0,1)similar-toğ‘¡ğ’°01t\\sim\\mathcal{U}(0,1)italic_t âˆ¼ caligraphic_U ( 0 , 1 )\n\nLogit-Normal (LN) Timestep Schedule\n\nThe Logit-Normal (LN) timestep schedule Ï€lâ¢nâ¢(t;m,s)subscriptğœ‹ğ‘™ğ‘›ğ‘¡ğ‘šğ‘ \\pi_{ln}(t;m,s)italic_Ï€ start_POSTSUBSCRIPT italic_l italic_n end_POSTSUBSCRIPT ( italic_t ; italic_m , italic_s ), proposed in Esser etÂ al. (2024), generates timesteps according to the following distribution:\n\nÏ€lâ¢nâ¢(t;m,s)=1sâ¢2â¢Ï€â‹…1tâ¢(1âˆ’t)â¢expâ¡(âˆ’(lâ¢oâ¢gâ¢iâ¢tâ¢(t)âˆ’m)22â¢s2),subscriptğœ‹ğ‘™ğ‘›ğ‘¡ğ‘šğ‘ â‹…1ğ‘ 2ğœ‹1ğ‘¡1ğ‘¡superscriptğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ğ‘¡ğ‘š22superscriptğ‘ 2\\pi_{ln}(t;m,s)=\\frac{1}{s\\sqrt{2\\pi}}\\cdot\\frac{1}{t(1-t)}\\exp\\left(-\\frac{(% logit(t)-m)^{2}}{2s^{2}}\\right),italic_Ï€ start_POSTSUBSCRIPT italic_l italic_n end_POSTSUBSCRIPT ( italic_t ; italic_m , italic_s ) = divide start_ARG 1 end_ARG start_ARG italic_s square-root start_ARG 2 italic_Ï€ end_ARG end_ARG â‹… divide start_ARG 1 end_ARG start_ARG italic_t ( 1 - italic_t ) end_ARG roman_exp ( - divide start_ARG ( italic_l italic_o italic_g italic_i italic_t ( italic_t ) - italic_m ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) ,\n\nwhere lâ¢oâ¢gâ¢iâ¢tâ¢(t)=logâ¡(t1âˆ’t)ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ğ‘¡ğ‘¡1ğ‘¡logit(t)=\\log\\left(\\frac{t}{1-t}\\right)italic_l italic_o italic_g italic_i italic_t ( italic_t ) = roman_log ( divide start_ARG italic_t end_ARG start_ARG 1 - italic_t end_ARG ). The LN schedule has two parameters: mğ‘šmitalic_m and sğ‘ sitalic_s. It defines an unimodal distribution, where mğ‘šmitalic_m controls the center of the distribution in logit space, shifting the emphasis of training samples towards noisier or cleaner regions. The parameter sğ‘ sitalic_s adjusts the spread of the distribution, determining its width. As suggested in Esser etÂ al. (2024), to obtain a timestep, we first sample uâˆ¼ğ’©â¢(m,s)similar-toğ‘¢ğ’©ğ‘šğ‘ u\\sim\\mathcal{N}(m,s)italic_u âˆ¼ caligraphic_N ( italic_m , italic_s ), and then transform it using the logistic function. For discrete-time diffusion, after obtaining tâˆ¼Ï€lâ¢nâ¢(t;m,s)similar-toğ‘¡subscriptğœ‹ğ‘™ğ‘›ğ‘¡ğ‘šğ‘ t\\sim\\pi_{ln}(t;m,s)italic_t âˆ¼ italic_Ï€ start_POSTSUBSCRIPT italic_l italic_n end_POSTSUBSCRIPT ( italic_t ; italic_m , italic_s ), we scale tğ‘¡titalic_t by t=roundâ¢(tÃ—999)ğ‘¡roundğ‘¡999t=\\text{round}(t\\times 999)italic_t = round ( italic_t Ã— 999 ) to obtain a discrete timestep. Following Esser etÂ al. (2024), we utilized the parameters m=0.0,s=1.00formulae-sequenceğ‘š0.0ğ‘ 1.00m=0.0,s=1.00italic_m = 0.0 , italic_s = 1.00 and didnâ€™t sweep over mğ‘šmitalic_m and sğ‘ sitalic_s. More details and visualizations can be found in Esser etÂ al. (2024) Appendix B.4.\n\nWe conducted a series of experiments using different combinations of formulations. Selective combinations are listed in Tab.Â 5. A â€™âˆ’\\mathbf{-}-â€™ indicates that the combination is either not comparable with other formulations or that training diverges. We assume that the choice of formulation will not be significantly affected by specific model designs or datasets. All experiments were conducted using Pixart (Chen etÂ al., 2023), a popular text-to-image diffusion transformer architecture. Specifically, we used a small model with 12 layers and a hidden size of 384, setting the patch size to 2. The models were trained on JourneyDB (Sun etÂ al., 2024), a medium-sized text-to-image dataset consisting of synthetic images collected from Midjourney. All models were trained for 400k steps using AdamW as the optimizer. As shown in Tab.Â 5, the optimal combination is [RF, LN, ğ¯ğ¯\\mathbf{v}bold_v], achieving an FID of 36.336 and a Clip Score of 0.26684. This combination achieved the best performance on both metrics and therefore, we used this setting in the remaining experiments.\n\n\nF.2 EMA\n\nThe Exponential Moving Average (EMA) coefficient is crucial to the loss curve and determining the final results. In EMA, the loss lğ‘™litalic_l is updated as l=(1âˆ’Î±EMA)â¢l+Î±EMAâ¢vğ‘™1subscriptğ›¼EMAğ‘™subscriptğ›¼EMAğ‘£l=(1-\\alpha_{\\text{EMA}})l+\\alpha_{\\text{EMA}}vitalic_l = ( 1 - italic_Î± start_POSTSUBSCRIPT EMA end_POSTSUBSCRIPT ) italic_l + italic_Î± start_POSTSUBSCRIPT EMA end_POSTSUBSCRIPT italic_v, where vğ‘£vitalic_v represents the latest loss value. EMA smooths the cumulative loss and reduces fluctuations. However, applying EMA can lead to an overestimation of the loss during the early stages of training. As illustrated in Fig.Â 7, a larger EMA coefficient results in a higher loss compared to the actual value, which may introduce significant bias in scaling curve fitting and, consequently, lead to inefficient use of computational resources. From Fig.Â 7, we observe that Î±EMA=0.9subscriptğ›¼EMA0.9\\alpha_{\\text{EMA}}=0.9italic_Î± start_POSTSUBSCRIPT EMA end_POSTSUBSCRIPT = 0.9 is the optimal choice, as it effectively smooths the loss curve while only slightly inflating the values during the initial phase of training.\n\n\nF.3 Classifier-Free Guidance & Sampling Steps\n\nWe perform ablation studies on CFG scales and the number of steps using the compute-optimal models trained with a budget of 6e18. In Fig.Â 8, we evaluate several CFG scales (2.5, 5.0, 7.5, 10.0) and compute FID across different step counts. We found that 25 steps are sufficient to achieve good results. Next, we fix the number of steps at 25 and evaluate performance across different CFG scales. As shown in Fig.Â 9, a CFG of 10.0 yields the best results, and we select this configuration as the default.\n\n",
    "tool_call_id": "call_tooluvrtx01QbjqVZ5iV"
  },
  {
    "timestamp": "2025-10-22T03:48:01.342Z",
    "event_type": "assistant_response",
    "user_id": "jiadisu7@gmail.com",
    "session_id": "43515b75-14c3-42dd-9420-b3fee5ce30d3",
    "response": "åŸºäºå¯¹DiTï¼ˆDiffusion Transformerï¼‰è®¡ç®—å¤æ‚åº¦çš„åˆ†æï¼Œæˆ‘å¯ä»¥å›ç­”ä½ å…³äºåºåˆ—é•¿åº¦ç¼©æ”¾çš„é—®é¢˜ï¼š\n\n**T_{2L} ä¸ 2*T_L çš„å…³ç³»ï¼š**\n\nåœ¨æ˜¾å­˜ä¸å—é™çš„æƒ…å†µä¸‹ï¼Œ**T_{2L} < 2*T_L**ï¼Œå³å¯¹é•¿åº¦ä¸º2Lçš„åºåˆ—åšå»å™ªçš„æ—¶é—´å°äºå¯¹ä¸¤ä¸ªé•¿åº¦ä¸ºLçš„åºåˆ—åˆ†åˆ«å»å™ªæ‰€éœ€æ—¶é—´çš„æ€»å’Œã€‚\n\n**åŸå› åˆ†æï¼š**\n\n1. **Transformerçš„äºŒæ¬¡å¤æ‚åº¦ç‰¹æ€§**ï¼š\n   - DiTåŸºäºTransformeræ¶æ„ï¼Œå…¶è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦ä¸ºO(LÂ²)\n   - å¯¹äºé•¿åº¦Lçš„åºåˆ—ï¼šè®¡ç®—å¤æ‚åº¦ âˆ LÂ²\n   - å¯¹äºé•¿åº¦2Lçš„åºåˆ—ï¼šè®¡ç®—å¤æ‚åº¦ âˆ (2L)Â² = 4LÂ²\n\n2. **å¹¶è¡Œè®¡ç®—ä¼˜åŠ¿**ï¼š\n   - å•ä¸ª2Låºåˆ—å¯ä»¥å……åˆ†åˆ©ç”¨GPUçš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›\n   - çŸ©é˜µè¿ç®—åœ¨æ›´å¤§çš„çŸ©é˜µä¸Šé€šå¸¸æœ‰æ›´å¥½çš„è®¡ç®—æ•ˆç‡\n   - å‡å°‘äº†å†…å­˜è®¿é—®çš„å¼€é”€å’Œkernelå¯åŠ¨çš„å¼€é”€\n\n3. **å…·ä½“å…³ç³»**ï¼š\n   - T_L âˆ LÂ²ï¼ˆåŠ ä¸Šçº¿æ€§é¡¹ï¼‰\n   - T_{2L} âˆ (2L)Â² = 4LÂ²\n   - 2*T_L âˆ 2*LÂ²\n   - å› æ­¤ï¼šT_{2L}/T_L â‰ˆ 4ï¼Œè€Œ 2*T_L/T_L = 2\n   - æ‰€ä»¥ï¼šT_{2L} â‰ˆ 2*(2*T_L) = 4*T_L > 2*T_L\n\n**ç­‰ç­‰ï¼Œè®©æˆ‘é‡æ–°åˆ†æï¼š**\n\nå®é™…ä¸Šï¼Œä»è®¡ç®—å¤æ‚åº¦è§’åº¦ï¼š\n- T_{2L} â‰ˆ 4*T_Lï¼ˆå› ä¸ºæ³¨æ„åŠ›å¤æ‚åº¦æ˜¯äºŒæ¬¡çš„ï¼‰\n- 2*T_L å°±æ˜¯ 2*T_L\n\næ‰€ä»¥ **T_{2L} â‰ˆ 2*(2*T_L)**ï¼Œå³å¤„ç†ä¸€ä¸ª2Lé•¿åº¦åºåˆ—çš„æ—¶é—´å¤§çº¦æ˜¯å¤„ç†ä¸¤ä¸ªLé•¿åº¦åºåˆ—æ€»æ—¶é—´çš„2å€ã€‚\n\nè¿™æ˜¯å› ä¸ºTransformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å…·æœ‰O(nÂ²)çš„è®¡ç®—å¤æ‚åº¦ï¼Œåºåˆ—é•¿åº¦ç¿»å€ä¼šå¯¼è‡´è®¡ç®—é‡å¢åŠ 4å€ã€‚",
    "model": "anthropic/claude-sonnet-4",
    "metadata": {
      "cli_version": "0.0.33",
      "platform": "linux"
    }
  },
  {
    "timestamp": "2025-10-22T03:48:01.405Z",
    "event_type": "system_event",
    "user_id": "jiadisu7@gmail.com",
    "session_id": "43515b75-14c3-42dd-9420-b3fee5ce30d3",
    "metadata": {
      "event_type": "session_context",
      "system_instruction": "You are SII Cli, an interactive CLI agent developed by GAIR-NLP, specializing in software engineering tasks. Your primary goal is to help users safely and efficiently, adhering strictly to the following instructions and utilizing your available tools.\n\n# Core Mandates\n\n- **Conventions:** Rigorously adhere to existing project conventions when reading or modifying code. Analyze surrounding code, tests, and configuration first.\n- **Libraries/Frameworks:** NEVER assume a library/framework is available or appropriate. Verify its established usage within the project (check imports, configuration files like 'package.json', 'Cargo.toml', 'requirements.txt', 'build.gradle', etc., or observe neighboring files) before employing it.\n- **Style & Structure:** Mimic the style (formatting, naming), structure, framework choices, typing, and architectural patterns of existing code in the project.\n- **Idiomatic Changes:** When editing, understand the local context (imports, functions/classes) to ensure your changes integrate naturally and idiomatically.\n- **Comments:** Add code comments sparingly. Focus on *why* something is done, especially for complex logic, rather than *what* is done. Only add high-value comments if necessary for clarity or if requested by the user. Do not edit comments that are separate from the code you are changing. *NEVER* talk to the user or describe your changes through comments.\n- **Proactiveness:** Fulfill the user's request thoroughly, including reasonable, directly implied follow-up actions.\n- **Confirm Ambiguity/Expansion:** Do not take significant actions beyond the clear scope of the request without confirming with the user. If asked *how* to do something, explain first, don't just do it.\n- **Explaining Changes:** After completing a code modification or file operation *do not* provide summaries unless asked.\n- **Path Construction:** Before using any file system tool (e.g., 'read_file' or 'write_file'), you must construct the full absolute path for the file_path argument. Always combine the absolute path of the project's root directory with the file's path relative to the root. For example, if the project root is /path/to/project/ and the file is foo/bar/baz.txt, the final path you must use is /path/to/project/foo/bar/baz.txt. If the user provides a relative path, you must resolve it against the root directory to create an absolute path.\n- **Do Not revert changes:** Do not revert changes to the codebase unless asked to do so by the user. Only revert changes made by you if they have resulted in an error or if the user has explicitly asked you to revert the changes.\n\n# Primary Workflows\n\n## Software Engineering Tasks\nWhen requested to perform tasks like fixing bugs, adding features, refactoring, or explaining code, follow this sequence:\n1. **Understand:** Think about the user's request and the relevant codebase context. Use 'search_file_content' and 'glob' search tools extensively (in parallel if independent) to understand file structures, existing code patterns, and conventions. Use 'read_file' and 'read_many_files' to understand context and validate any assumptions you may have.\n2. **Plan:** Build a coherent and grounded (based on the understanding in step 1) plan for how you intend to resolve the user's task. Share an extremely concise yet clear plan with the user if it would help the user understand your thought process. As part of the plan, you should try to use a self-verification loop by writing unit tests if relevant to the task. Use output logs or debug statements as part of this self verification loop to arrive at a solution.\n3. **Implement:** Use the available tools (e.g., 'replace', 'write_file' 'run_shell_command' ...) to act on the plan, strictly adhering to the project's established conventions (detailed under 'Core Mandates').\n4. **Verify (Tests):** If applicable and feasible, verify the changes using the project's testing procedures. Identify the correct test commands and frameworks by examining 'README' files, build/package configuration (e.g., 'package.json'), or existing test execution patterns. NEVER assume standard test commands.\n5. **Verify (Standards):** VERY IMPORTANT: After making code changes, execute the project-specific build, linting and type-checking commands (e.g., 'tsc', 'npm run lint', 'ruff check .') that you have identified for this project (or obtained from the user). This ensures code quality and adherence to standards. If unsure about these commands, you can ask the user if they'd like you to run them and if so how to.\n\n## New Applications\n\n**Goal:** Autonomously implement and deliver a visually appealing, substantially complete, and functional prototype. Utilize all tools at your disposal to implement the application. Some tools you may especially find useful are 'write_file', 'replace' and 'run_shell_command'.\n\n1. **Understand Requirements:** Analyze the user's request to identify core features, desired user experience (UX), visual aesthetic, application type/platform (web, mobile, desktop, CLI, library, 2D or 3D game), and explicit constraints. If critical information for initial planning is missing or ambiguous, ask concise, targeted clarification questions.\n2. **Propose Plan:** Formulate an internal development plan. Present a clear, concise, high-level summary to the user. This summary must effectively convey the application's type and core purpose, key technologies to be used, main features and how users will interact with them, and the general approach to the visual design and user experience (UX) with the intention of delivering something beautiful, modern, and polished, especially for UI-based applications. For applications requiring visual assets (like games or rich UIs), briefly describe the strategy for sourcing or generating placeholders (e.g., simple geometric shapes, procedurally generated patterns, or open-source assets if feasible and licenses permit) to ensure a visually complete initial prototype. Ensure this information is presented in a structured and easily digestible manner.\n  - When key technologies aren't specified, prefer the following:\n  - **Websites (Frontend):** React (JavaScript/TypeScript) with Bootstrap CSS, incorporating Material Design principles for UI/UX.\n  - **Back-End APIs:** Node.js with Express.js (JavaScript/TypeScript) or Python with FastAPI.\n  - **Full-stack:** Next.js (React/Node.js) using Bootstrap CSS and Material Design principles for the frontend, or Python (Django/Flask) for the backend with a React/Vue.js frontend styled with Bootstrap CSS and Material Design principles.\n  - **CLIs:** Python or Go.\n  - **Mobile App:** Compose Multiplatform (Kotlin Multiplatform) or Flutter (Dart) using Material Design libraries and principles, when sharing code between Android and iOS. Jetpack Compose (Kotlin JVM) with Material Design principles or SwiftUI (Swift) for native apps targeted at either Android or iOS, respectively.\n  - **3d Games:** HTML/CSS/JavaScript with Three.js.\n  - **2d Games:** HTML/CSS/JavaScript.\n3. **User Approval:** Obtain user approval for the proposed plan.\n4. **Implementation:** Autonomously implement each feature and design element per the approved plan utilizing all available tools. When starting ensure you scaffold the application using 'run_shell_command' for commands like 'npm init', 'npx create-react-app'. Aim for full scope completion. Proactively create or source necessary placeholder assets (e.g., images, icons, game sprites, 3D models using basic primitives if complex assets are not generatable) to ensure the application is visually coherent and functional, minimizing reliance on the user to provide these. If the model can generate simple assets (e.g., a uniformly colored square sprite, a simple 3D cube), it should do so. Otherwise, it should clearly indicate what kind of placeholder has been used and, if absolutely necessary, what the user might replace it with. Use placeholders only when essential for progress, intending to replace them with more refined versions or instruct the user on replacement during polishing if generation is not feasible.\n5. **Verify:** Review work against the original request, the approved plan. Fix bugs, deviations, and all placeholders where feasible, or ensure placeholders are visually adequate for a prototype. Ensure styling, interactions, produce a high-quality, functional and beautiful prototype aligned with design goals. Finally, but MOST importantly, build the application and ensure there are no compile errors.\n6. **Solicit Feedback:** If still applicable, provide instructions on how to start the application and request user feedback on the prototype.\n\n## Sub-Agent Orchestration (Prompt-based)\n\nYou can call the Agent Tool (sub-agents) when a focused specialist would accomplish the task faster or more reliably. Prefer prompt-based orchestration over programmatic routing.\n\n### Agent Types and When to Use\n- Architect (agentType: \"architect\")\n  - Use when the repository is large/unknown or when you need a structural map before acting\n  - Deliverable: docs/architecture.md with repo map (directories, key modules, data flows, dependency hotspots)\n  - Typical tools: 'read_file', 'search_file_content', 'glob', 'write_file'\n  - Do not modify code; write a single architecture artifact and stop\n\n- SpecFlow (agentType: \"specflow\")\n  - Use for end-to-end feature/bug work with Plan â†’ Review â†’ Execute workflow\n  - Deliverable: concrete plan, minimal safe diffs, and verification notes; avoid shell by default\n  - Typical tools: 'read_file', 'search_file_content', 'glob', 'write_file'\n\n- Refactor (agentType: \"refactor\")\n  - Use for pattern-wide changes across many files with strict consistency and small batches\n  - Deliverable: a set of precise edits with rationale and rollback notes\n  - Typical tools: 'read_file', 'search_file_content', 'glob', 'write_file'\n\n- Researcher (agentType: \"researcher\")\n  - Use when external knowledge is required (APIs, libraries, benchmarks)\n  - Deliverable: docs/research/<topic>.md with sources and syntheses\n  - Typical tools: web_search, web_fetch, 'read_file', 'write_file'\n\n- Debug Analyzer (agentType: \"debug-analyzer\")\n  - Use for systematic problem investigation, error analysis, and troubleshooting\n  - Deliverable: comprehensive debugging report with root cause analysis and solution verification\n  - Typical tools: 'read_file', 'search_file_content', 'glob', 'write_file'\n  - Focus on evidence-based analysis and maintaining system stability during investigation\n\n### Orchestration Patterns\n- Large/Unknown Repo:\n  1) Call Architect to create docs/architecture.md (or update it). 2) Then hand off to SpecFlow with the concrete plan referencing that artifact.\n- Pattern-wide Change:\n  Use Refactor with a crisp spec and examples; prefer batched changes with verification between batches.\n- External Knowledge Needed:\n  Call Researcher first; archive findings, then proceed with SpecFlow/Refactor.\n\n### Natural Stop (No Hard Limits)\n- Stop when the plan is fully executed or after two consecutive no-op steps (no new diffs/findings).\n- If the task grows large, save the current stage artifact (plan/architecture/refactor batch) and stop with next steps.\n\n### How to Invoke\nAlways pass an explicit agentType and a crisp task. Example (pseudocode):\n- agent_tool.execute({ agentType: \"architect\", task: \"Map repo and write docs/architecture.md\" })\n- agent_tool.execute({ agentType: \"specflow\", task: \"Implement Plan â†’ Review â†’ Execute for feature X\" })\n- agent_tool.execute({ agentType: \"refactor\", task: \"Apply consistent rename across modules with tests updated\" })\n- agent_tool.execute({ agentType: \"researcher\", task: \"Compare libraries A vs B; save findings\" })\n- agent_tool.execute({ agentType: \"debug-analyzer\", task: \"Investigate error X, identify root cause and verify solution\" })\n\n# Operational Guidelines\n\n## Tone and Style (CLI Interaction)\n- **Concise & Direct:** Adopt a professional, direct, and concise tone suitable for a CLI environment.\n- **Minimal Output:** Aim for fewer than 3 lines of text output (excluding tool use/code generation) per response whenever practical. Focus strictly on the user's query.\n- **Clarity over Brevity (When Needed):** While conciseness is key, prioritize clarity for essential explanations or when seeking necessary clarification if a request is ambiguous.\n- **No Chitchat:** Avoid conversational filler, preambles (\"Okay, I will now...\"), or postambles (\"I have finished the changes...\"). Get straight to the action or answer.\n- **Formatting:** Use GitHub-flavored Markdown. Responses will be rendered in monospace.\n- **Tools vs. Text:** Use tools for actions, text output *only* for communication. Do not add explanatory comments within tool calls or code blocks unless specifically part of the required code/command itself.\n- **Handling Inability:** If unable/unwilling to fulfill a request, state so briefly (1-2 sentences) without excessive justification. Offer alternatives if appropriate.\n\n## Interaction Details\n- **Help Command:** The user can use '/help' to display help information.\n- **Feedback:** To report a bug or provide feedback, please use the /bug command.\n\n# Security and Safety Rules\n\n- **Explain Critical Commands:** Before executing commands with 'run_shell_command' that modify the file system, codebase, or system state, you *must* provide a brief explanation of the command's purpose and potential impact. Prioritize user understanding and safety. You should not ask permission to use the tool; the user will be presented with a confirmation dialogue upon use (you do not need to tell them this).\n- **Security First:** Always apply security best practices. Never introduce code that exposes, logs, or commits secrets, API keys, or other sensitive information.\n\n## Tool Usage\n- **File Paths:** Always use absolute paths when referring to files with tools like 'read_file' or 'write_file'. Relative paths are not supported. You must provide an absolute path.\n- **Parallelism:** Execute multiple independent tool calls in parallel when feasible (i.e. searching the codebase).\n- **Command Execution:** Use the 'run_shell_command' tool for running shell commands, remembering the safety rule to explain modifying commands first.\n- **Background Processes:** Use background processes (via `&`) for commands that are unlikely to stop on their own, e.g. `node server.js &`. If unsure, ask the user.\n- **Interactive Commands:** Try to avoid shell commands that are likely to require user interaction (e.g. `git rebase -i`). Use non-interactive versions of commands (e.g. `npm init -y` instead of `npm init`) when available, and otherwise remind the user that interactive shell commands are not supported and may cause hangs until canceled by the user.\n- **Remembering Facts:** Use the 'save_memory' tool to remember specific, *user-related* facts or preferences when the user explicitly asks, or when they state a clear, concise piece of information that would help personalize or streamline *your future interactions with them* (e.g., preferred coding style, common project paths they use, personal tool aliases). This tool is for user-specific information that should persist across sessions. Do *not* use it for general project context or information that belongs in project-specific `GEMINI.md` files. If unsure whether to save something, you can ask the user, \"Should I remember that for you?\"\n- **SII Cognitions (When Available):** When authenticated with SII, you have access to a powerful cognitions tool that allows you to search and retrieve knowledge from the SII cognitions database. \"Cognitions\" are structured knowledge entries containing questions, answers, abstracts, and metadata about various topics, particularly in AI, engineering, and technology domains. Use this tool when users ask about technical concepts, research topics, or when you need to provide authoritative information that might be available in the cognitions database. The tool supports searching by keywords, filtering by topics, time periods, and authors, as well as retrieving detailed information about specific cognitions by ID.\n- **SII Deep Research (When Available):** When authenticated with SII, you have access to a comprehensive deep research tool that conducts multi-step research including web search, cognition analysis, and document generation. CRITICAL BEHAVIORAL GUIDELINES: (1) Deep research typically takes 5-10 minutes to complete - ALWAYS inform users about this expected duration upfront. (2) Suggest users can handle other tasks while waiting. (3) DO NOT automatically check progress - wait for the user to explicitly request a status update. (4) When user requests status, check only ONCE per request, then inform the user and stop. (5) The tool provides real-time progress updates during execution when checked. (6) Wait for natural completion rather than forcing early termination unless specifically requested by the user. (7) When starting research, set proper expectations: \"This research will take approximately 5-10 minutes. I'll wait for you to ask for updates if needed. Feel free to work on other tasks in the meantime.\" MANDATORY WORKFLOW EXAMPLE: Step 1: Start research with time expectation message. Step 2: Wait for user to explicitly request status update. Step 3: When requested, check status ONCE and report back to user. Step 4: Only retrieve results when research shows completion or user requests results. DO NOT proactively check status. Only check when explicitly asked by the user, and only once per user request.\n\n- IMPORTANT ADDITION (behavior when user explicitly asks progress): When the user explicitly requests research progress/status and you perform the single phase analysis check required by the SII rules, if that single check shows the research is complete (for example, 'phase_progress' equals 100, or 'current_phase' is \"completion\" or \"session_end\"), then instead of only replying in natural language asking whether to save the results, automatically initiate the deep research tool's save-results flow by invoking the SII deep research tool with 'action: \"save_results\"' for that session. This invocation should surface the tool's built-in confirmation UI so the user can accept or cancel saving the full report. Always respect the confirmation outcome and never write the file without explicit user confirmation. This automatic save-results tool call is permitted only after the user requested the status check and only when the single check shows completion; continue to follow the other SII rules (check once per request, inform about expected duration, avoid polling, and let users work on other tasks while waiting).\n\n- **Respect User Confirmations:** Most tool calls (also denoted as 'function calls') will first require confirmation from the user, where they will either approve or cancel the function call. If a user cancels a function call, respect their choice and do _not_ try to make the function call again. It is okay to request the tool call again _only_ if the user requests that same tool call on a subsequent prompt. When a user cancels a function call, assume best intentions from the user and consider inquiring if they prefer any alternative paths forward.\n\n# Outside of Sandbox\nYou are running outside of a sandbox container, directly on the user's system. For critical commands that are particularly likely to modify the user's system outside of the project directory or system temp directory, as you explain the command to the user (per the Explain Critical Commands rule above), also remind the user to consider enabling sandboxing.\n\n# Git Repository\n- The current working (project) directory is being managed by a git repository.\n- When asked to commit changes or prepare a commit, always start by gathering information using shell commands:\n  - `git status` to ensure that all relevant files are tracked and staged, using `git add ...` as needed.\n  - `git diff HEAD` to review all changes (including unstaged changes) to tracked files in work tree since last commit.\n    - `git diff --staged` to review only staged changes when a partial commit makes sense or was requested by the user.\n  - `git log -n 3` to review recent commit messages and match their style (verbosity, formatting, signature line, etc.)\n- Combine shell commands whenever possible to save time/steps, e.g. `git status && git diff HEAD && git log -n 3`.\n- Always propose a draft commit message. Never just ask the user to give you the full commit message.\n- Prefer commit messages that are clear, concise, and focused more on \"why\" and less on \"what\".\n- Keep the user informed and ask for clarification or confirmation where needed.\n- After each commit, confirm that it was successful by running `git status`.\n- If a commit fails, never attempt to work around the issues without being asked to do so.\n- Never push changes to a remote repository without being asked explicitly by the user.\n\n# Examples (Illustrating Tone and Workflow)\n<example>\nuser: 1 + 2\nmodel: 3\n</example>\n\n<example>\nuser: is 13 a prime number?\nmodel: true\n</example>\n\n<example>\nuser: start the server implemented in server.js\nmodel: \n<tool_call>\n<function=run_shell_command>\n<parameter=command>\nnode server.js &\n</parameter>\n</function>\n</tool_call>\n</example>\n\n<example>\nuser: Refactor the auth logic in src/auth.py to use the requests library instead of urllib.\nmodel: Okay, I can refactor 'src/auth.py'.\nFirst, I'll analyze the code and check for a test safety net before planning any changes.\n<tool_call>\n<function=glob>\n<parameter=path>\ntests/test_auth.py\n</parameter>\n</function>\n</tool_call>\n<tool_call>\n<function=read_file>\n<parameter=path>\n/path/to/tests/test_auth.py\n</parameter>\n<parameter=offset>\n0\n</parameter>\n<parameter=limit>\n10\n</parameter>\n</function>\n</tool_call>\n(After analysis)\nGreat, 'tests/test_auth.py' exists and covers the core authentication logic. With this safety net in place, I can safely plan the refactoring.\nI'll also confirm 'requests' is a dependency.\n<tool_call>\n<function=read_file>\n<parameter=path>\n/path/to/requirements.txt\n</parameter>\n</function>\n</tool_call>\n(After analysis)\nLooks good, 'requests' is available.\n\nHere's the plan:\n1.  Replace the 'urllib' calls with 'requests'.\n2.  Add proper 'try...except' error handling for the new network calls.\n3.  Remove the old 'urllib' import.\n4.  Run the project's linter and tests to verify the changes.\n\n<tool_call>\n<function=replace>\n<parameter=path>\nsrc/auth.py\n</parameter>\n<parameter=old_content>\n(old code content)\n</parameter>\n<parameter=new_content>\n(new code content)\n</parameter>\n</function>\n</tool_call>\nRefactoring complete. Running verification...\n<tool_call>\n<function=run_shell_command\n<parameter=command>\nruff check src/auth.py && pytest\n</parameter>\n</function>\n</tool_call>\n(After verification passes)\nAll checks passed. This is a stable checkpoint.\nWould you like me to write a commit message and commit these changes?\n</example>\n\n<example>\nuser: Delete the temp directory.\nmodel: I can run `rm -rf /path/to/project/temp`. This will permanently delete the directory and all its contents.\n</example>\n\n<example>\nuser: Write tests for someFile.ts\nmodel:\nOkay, I can write those tests. First, I'll read someFile.ts to understand its functionality.\n<tool_call>\n<function=read_file>\n<parameter=path>\n/path/to/someFile.ts\n</parameter>\n</function>\n</tool_call>\nNow I'll look for existing or related test files to understand current testing conventions and dependencies.\n<tool_call>\n<function>read_many_files for paths ['**/*.test.ts', 'src/**/*.spec.ts'] assuming someFile.ts is in the src directory]\n</tool_call>\n(After reviewing existing tests and the file content)\n<tool_call>\n<function=write_file>\n<parameter=path>\n/path/to/someFile.test.ts\n</parameter>\n</function>\n</tool_call>\nI've written the tests. Now I'll run the project's test command to verify them.\n<tool_call>\n<function=run_shell_command>\n<parameter=command>\nnpm run test\n</parameter>\n</function>\n</tool_call>\n(After verification passes)\nAll checks passed. This is a stable checkpoint.\n</example>\n\n<example>\nuser: Where are all the 'app.config' files in this project? I need to check their settings.\nmodel:\n<tool_call>\n<function=glob>\n<parameter=pattern>\n./**/app.config\n</parameter>\n</function>\n</tool_call>\n(Assuming GlobTool returns a list of paths like ['/path/to/moduleA/app.config', '/path/to/moduleB/app.config'])\nI found the following 'app.config' files:\n- /path/to/moduleA/app.config\n- /path/to/moduleB/app.config\nTo help you check their settings, I can read their contents. Which one would you like to start with, or should I read all of them?\n</example>\n\n# Final Reminder\n\nYour core function is efficient and safe assistance. Balance extreme conciseness with the crucial need for clarity, especially regarding safety and potential system modifications. Always prioritize user control and project conventions. Never make assumptions about the contents of files; instead use 'read_file' or 'read_many_files' to ensure you aren't making broad assumptions. Finally, you are an agent - please keep going until the user's query is completely resolved.",
      "available_tools": [
        {
          "name": "agent_tool",
          "description": "Creates and manages specialized sub-agents for complex task delegation with controlled tool access and context isolation.",
          "parameters": {
            "type": "OBJECT",
            "properties": {
              "agentType": {
                "type": "STRING",
                "description": "Type of sub-agent to create. Available types: ",
                "enum": []
              },
              "agentDescriptions": {
                "type": "OBJECT",
                "description": "Descriptions of available agent types",
                "properties": {}
              },
              "task": {
                "type": "STRING",
                "description": "Specific task for the sub-agent to complete. Be clear and detailed about what you want the sub-agent to accomplish."
              },
              "context": {
                "type": "STRING",
                "description": "Level of context sharing with sub-agent. \"partial\" shares relevant context, \"full\" shares complete conversation, \"minimal\" shares only the task.",
                "enum": [
                  "full",
                  "partial",
                  "minimal"
                ],
                "default": "partial"
              },
              "maxTurns": {
                "type": "INTEGER",
                "description": "Maximum conversation turns for the sub-agent. Higher values allow more complex tasks but use more resources.",
                "minimum": 1,
                "maximum": 50,
                "default": 15
              },
              "timeoutMs": {
                "type": "INTEGER",
                "description": "Timeout in milliseconds for sub-agent execution. Longer timeouts allow more complex tasks.",
                "minimum": 30000,
                "maximum": 1800000,
                "default": 300000
              },
              "priority": {
                "type": "STRING",
                "description": "Priority level for resource allocation. Higher priority agents get more resources.",
                "enum": [
                  "high",
                  "normal",
                  "low"
                ],
                "default": "normal"
              }
            },
            "required": [
              "agentType",
              "task"
            ]
          }
        },
        {
          "name": "replace",
          "description": "Replaces text within a file. By default, replaces a single occurrence, but can replace multiple occurrences when `expected_replacements` is specified. This tool requires providing significant context around the change to ensure precise targeting. Always use the read_file tool to examine the file's current content before attempting a text replacement.\n\n      The user has the ability to modify the `new_string` content. If modified, this will be stated in the response.\n\nExpectation for required parameters:\n1. `file_path` MUST be an absolute path; otherwise an error will be thrown.\n2. `old_string` MUST be the exact literal text to replace (including all whitespace, indentation, newlines, and surrounding code etc.).\n3. `new_string` MUST be the exact literal text to replace `old_string` with (also including all whitespace, indentation, newlines, and surrounding code etc.). Ensure the resulting code is correct and idiomatic.\n4. NEVER escape `old_string` or `new_string`, that would break the exact literal text requirement.\n**Important:** If ANY of the above are not satisfied, the tool will fail. CRITICAL for `old_string`: Must uniquely identify the single instance to change. Include at least 3 lines of context BEFORE and AFTER the target text, matching whitespace and indentation precisely. If this string matches multiple locations, or does not match exactly, the tool will fail.\n**Multiple replacements:** Set `expected_replacements` to the number of occurrences you want to replace. The tool will replace ALL occurrences that match `old_string` exactly. Ensure the number of replacements matches your expectation.",
          "parameters": {
            "properties": {
              "file_path": {
                "description": "The absolute path to the file to modify. Must start with '/'.",
                "type": "STRING"
              },
              "old_string": {
                "description": "The exact literal text to replace, preferably unescaped. For single replacements (default), include at least 3 lines of context BEFORE and AFTER the target text, matching whitespace and indentation precisely. For multiple replacements, specify expected_replacements parameter. If this string is not the exact literal text (i.e. you escaped it) or does not match exactly, the tool will fail.",
                "type": "STRING"
              },
              "new_string": {
                "description": "The exact literal text to replace `old_string` with, preferably unescaped. Provide the EXACT text. Ensure the resulting code is correct and idiomatic.",
                "type": "STRING"
              },
              "expected_replacements": {
                "type": "NUMBER",
                "description": "Number of replacements expected. Defaults to 1 if not specified. Use when you want to replace multiple occurrences.",
                "minimum": 1
              }
            },
            "required": [
              "file_path",
              "old_string",
              "new_string"
            ],
            "type": "OBJECT"
          }
        },
        {
          "name": "glob",
          "description": "Efficiently finds files matching specific glob patterns (e.g., `src/**/*.ts`, `**/*.md`), returning absolute paths sorted by modification time (newest first). Ideal for quickly locating files based on their name or path structure, especially in large codebases.",
          "parameters": {
            "properties": {
              "pattern": {
                "description": "The glob pattern to match against (e.g., '**/*.py', 'docs/*.md').",
                "type": "STRING"
              },
              "path": {
                "description": "Optional: The absolute path to the directory to search within. If omitted, searches the root directory.",
                "type": "STRING"
              },
              "case_sensitive": {
                "description": "Optional: Whether the search should be case-sensitive. Defaults to false.",
                "type": "BOOLEAN"
              },
              "respect_git_ignore": {
                "description": "Optional: Whether to respect .gitignore patterns when finding files. Only available in git repositories. Defaults to true.",
                "type": "BOOLEAN"
              }
            },
            "required": [
              "pattern"
            ],
            "type": "OBJECT"
          }
        },
        {
          "name": "read_file",
          "description": "Reads and returns the content of a specified file from the local filesystem. Handles text, images (PNG, JPG, GIF, WEBP, SVG, BMP), and PDF files. For text files, it can read specific line ranges.",
          "parameters": {
            "properties": {
              "absolute_path": {
                "description": "The absolute path to the file to read (e.g., '/home/user/project/file.txt'). Relative paths are not supported. You must provide an absolute path.",
                "type": "STRING"
              },
              "offset": {
                "description": "Optional: For text files, the 0-based line number to start reading from. Requires 'limit' to be set. Use for paginating through large files.",
                "type": "NUMBER"
              },
              "limit": {
                "description": "Optional: For text files, maximum number of lines to read. Use with 'offset' to paginate through large files. If omitted, reads the entire file (if feasible, up to a default limit).",
                "type": "NUMBER"
              }
            },
            "required": [
              "absolute_path"
            ],
            "type": "OBJECT"
          }
        },
        {
          "name": "list_directory",
          "description": "Lists the names of files and subdirectories directly within a specified directory path. Can optionally ignore entries matching provided glob patterns.",
          "parameters": {
            "properties": {
              "path": {
                "description": "The absolute path to the directory to list (must be absolute, not relative)",
                "type": "STRING"
              },
              "ignore": {
                "description": "List of glob patterns to ignore",
                "items": {
                  "type": "STRING"
                },
                "type": "ARRAY"
              },
              "file_filtering_options": {
                "description": "Optional: Whether to respect ignore patterns from .gitignore or .geminiignore",
                "type": "OBJECT",
                "properties": {
                  "respect_git_ignore": {
                    "description": "Optional: Whether to respect .gitignore patterns when listing files. Only available in git repositories. Defaults to true.",
                    "type": "BOOLEAN"
                  },
                  "respect_gemini_ignore": {
                    "description": "Optional: Whether to respect .geminiignore patterns when listing files. Defaults to true.",
                    "type": "BOOLEAN"
                  }
                }
              }
            },
            "required": [
              "path"
            ],
            "type": "OBJECT"
          }
        },
        {
          "name": "read_many_files",
          "description": "Reads content from multiple files specified by paths or glob patterns within a configured target directory. For text files, it concatenates their content into a single string. It is primarily designed for text-based files. However, it can also process image (e.g., .png, .jpg) and PDF (.pdf) files if their file names or extensions are explicitly included in the 'paths' argument. For these explicitly requested non-text files, their data is read and included in a format suitable for model consumption (e.g., base64 encoded).\n\nThis tool is useful when you need to understand or analyze a collection of files, such as:\n- Getting an overview of a codebase or parts of it (e.g., all TypeScript files in the 'src' directory).\n- Finding where specific functionality is implemented if the user asks broad questions about code.\n- Reviewing documentation files (e.g., all Markdown files in the 'docs' directory).\n- Gathering context from multiple configuration files.\n- When the user asks to \"read all files in X directory\" or \"show me the content of all Y files\".\n\nUse this tool when the user's query implies needing the content of several files simultaneously for context, analysis, or summarization. For text files, it uses default UTF-8 encoding and a '--- {filePath} ---' separator between file contents. Ensure paths are relative to the target directory. Glob patterns like 'src/**/*.js' are supported. Avoid using for single files if a more specific single-file reading tool is available, unless the user specifically requests to process a list containing just one file via this tool. Other binary files (not explicitly requested as image/PDF) are generally skipped. Default excludes apply to common non-text files (except for explicitly requested images/PDFs) and large dependency directories unless 'useDefaultExcludes' is false.",
          "parameters": {
            "type": "OBJECT",
            "properties": {
              "paths": {
                "type": "ARRAY",
                "items": {
                  "type": "STRING",
                  "minLength": "1"
                },
                "minItems": "1",
                "description": "Required. An array of glob patterns or paths relative to the tool's target directory. Examples: ['src/**/*.ts'], ['README.md', 'docs/']"
              },
              "include": {
                "type": "ARRAY",
                "items": {
                  "type": "STRING",
                  "minLength": "1"
                },
                "description": "Optional. Additional glob patterns to include. These are merged with `paths`. Example: [\"*.test.ts\"] to specifically add test files if they were broadly excluded.",
                "default": []
              },
              "exclude": {
                "type": "ARRAY",
                "items": {
                  "type": "STRING",
                  "minLength": "1"
                },
                "description": "Optional. Glob patterns for files/directories to exclude. Added to default excludes if useDefaultExcludes is true. Example: [\"**/*.log\", \"temp/\"]",
                "default": []
              },
              "recursive": {
                "type": "BOOLEAN",
                "description": "Optional. Whether to search recursively (primarily controlled by `**` in glob patterns). Defaults to true.",
                "default": true
              },
              "useDefaultExcludes": {
                "type": "BOOLEAN",
                "description": "Optional. Whether to apply a list of default exclusion patterns (e.g., node_modules, .git, binary files). Defaults to true.",
                "default": true
              },
              "file_filtering_options": {
                "description": "Whether to respect ignore patterns from .gitignore or .geminiignore",
                "type": "OBJECT",
                "properties": {
                  "respect_git_ignore": {
                    "description": "Optional: Whether to respect .gitignore patterns when listing files. Only available in git repositories. Defaults to true.",
                    "type": "BOOLEAN"
                  },
                  "respect_gemini_ignore": {
                    "description": "Optional: Whether to respect .geminiignore patterns when listing files. Defaults to true.",
                    "type": "BOOLEAN"
                  }
                }
              },
              "max_total_files": {
                "type": "INTEGER",
                "description": "Optional: Maximum number of files to read before truncating the result. Defaults to a safe guard value.",
                "minimum": 0,
                "default": 20
              },
              "max_total_chars": {
                "type": "INTEGER",
                "description": "Optional: Maximum total characters to return across all files. Defaults to a safe guard value.",
                "minimum": 0,
                "default": 200000
              },
              "per_file_max_lines": {
                "type": "INTEGER",
                "description": "Optional: Maximum number of lines to read from each text file. Defaults to a safe guard value.",
                "minimum": 0,
                "default": 200
              },
              "per_file_max_chars": {
                "type": "INTEGER",
                "description": "Optional: Maximum characters to read from each text file. Defaults to a safe guard value.",
                "minimum": 0,
                "default": 20000
              },
              "prefer_full_read": {
                "type": "BOOLEAN",
                "description": "Optional: Set to true after confirming the target files are small to disable per-file truncation guards. Total limits (max_total_files, max_total_chars) remain in effect unless overridden.",
                "default": false
              }
            },
            "required": [
              "paths"
            ]
          }
        },
        {
          "name": "save_memory",
          "description": "\nSaves a specific piece of information or fact to your long-term memory.\n\nUse this tool:\n\n- When the user explicitly asks you to remember something (e.g., \"Remember that I like pineapple on pizza\", \"Please save this: my cat's name is Whiskers\").\n- When the user states a clear, concise fact about themselves, their preferences, or their environment that seems important for you to retain for future interactions to provide a more personalized and effective assistance.\n\nDo NOT use this tool:\n\n- To remember conversational context that is only relevant for the current session.\n- To save long, complex, or rambling pieces of text. The fact should be relatively short and to the point.\n- If you are unsure whether the information is a fact worth remembering long-term. If in doubt, you can ask the user, \"Should I remember that for you?\"\n\n## Parameters\n\n- `fact` (string, required): The specific fact or piece of information to remember. This should be a clear, self-contained statement. For example, if the user says \"My favorite color is blue\", the fact would be \"My favorite color is blue\".\n",
          "parameters": {
            "type": "OBJECT",
            "properties": {
              "fact": {
                "type": "STRING",
                "description": "The specific fact or piece of information to remember. Should be a clear, self-contained statement."
              }
            },
            "required": [
              "fact"
            ]
          }
        },
        {
          "name": "search_file_content",
          "description": "Searches for a regular expression pattern within the content of files in a specified directory (or current working directory). Can filter files by a glob pattern. Returns the lines containing matches, along with their file paths and line numbers.",
          "parameters": {
            "properties": {
              "pattern": {
                "description": "The regular expression (regex) pattern to search for within file contents (e.g., 'function\\s+myFunction', 'import\\s+\\{.*\\}\\s+from\\s+.*').",
                "type": "STRING"
              },
              "path": {
                "description": "Optional: The absolute path to the directory to search within. If omitted, searches the current working directory.",
                "type": "STRING"
              },
              "include": {
                "description": "Optional: Array of glob patterns to filter which files are searched (e.g., ['*.js', '*.ts'], ['*.{ts,tsx}', 'src/**']). Each pattern is searched separately and results are merged. If omitted, searches all files (respecting potential global ignores).",
                "type": "ARRAY",
                "items": {
                  "type": "STRING",
                  "minLength": "1"
                }
              },
              "limit": {
                "description": "Optional: Maximum number of results to return per file pattern (default: 20). Helps prevent overwhelming results for broad searches.",
                "type": "NUMBER"
              },
              "totalLimit": {
                "description": "Optional: Maximum total number of results across all patterns (default: 100). Provides overall result limit regardless of pattern count.",
                "type": "NUMBER"
              },
              "offset": {
                "description": "Optional: Number of results to skip for pagination (default: 0). Use with limit to implement pagination: offset=0,limit=20 for page 1, offset=20,limit=20 for page 2.",
                "type": "NUMBER"
              }
            },
            "required": [
              "pattern"
            ],
            "type": "OBJECT"
          }
        },
        {
          "name": "run_shell_command",
          "description": "This tool executes a given shell command as `bash -c <command>`. Command can start background processes using `&`. Command is executed as a subprocess that leads its own process group. Command process group can be terminated as `kill -- -PGID` or signaled as `kill -s SIGNAL -- -PGID`.\n\n      The following information is returned:\n\n      Command: Executed command.\n      Directory: Directory (relative to project root) where command was executed, or `(root)`.\n      Stdout: Output on stdout stream. Can be `(empty)` or partial on error and for any unwaited background processes.\n      Stderr: Output on stderr stream. Can be `(empty)` or partial on error and for any unwaited background processes.\n      Error: Error or `(none)` if no error was reported for the subprocess.\n      Exit Code: Exit code or `(none)` if terminated by signal.\n      Signal: Signal number or `(none)` if no signal was received.\n      Background PIDs: List of background processes started or `(none)`.\n      Process Group PGID: Process group started or `(none)`",
          "parameters": {
            "type": "OBJECT",
            "properties": {
              "command": {
                "type": "STRING",
                "description": "Exact bash command to execute as `bash -c <command>`"
              },
              "description": {
                "type": "STRING",
                "description": "Brief description of the command for the user. Be specific and concise. Ideally a single sentence. Can be up to 3 sentences for clarity. No line breaks."
              },
              "directory": {
                "type": "STRING",
                "description": "(OPTIONAL) Directory to run the command in, if not the project root directory. Must be relative to the project root directory and must already exist."
              }
            },
            "required": [
              "command"
            ]
          }
        },
        {
          "name": "sii_cognition_extract_upload",
          "description": "Extract cognition information from web URLs (including YouTube videos) and upload to SII backend. Automatically analyzes content, generates structured cognition data with bilingual support, saves to the cognition database, and optionally saves raw content to local files. Supports YouTube transcript extraction and various content types. Only available when authenticated with SII.",
          "parameters": {
            "type": "OBJECT",
            "properties": {
              "url": {
                "type": "STRING",
                "description": "The URL to extract cognition from. Must be a valid HTTP/HTTPS URL."
              },
              "source": {
                "type": "STRING",
                "description": "Optional description of the content source (e.g., \"æŠ€æœ¯åšå®¢\", \"å­¦æœ¯è®ºæ–‡\")."
              },
              "custom_prompt": {
                "type": "STRING",
                "description": "Optional custom prompt to guide the extraction process (e.g., \"é‡ç‚¹å…³æ³¨æŠ€æœ¯ç»†èŠ‚\")."
              },
              "language_preference": {
                "type": "STRING",
                "description": "Language preference for output: \"zh\" (Chinese), \"en\" (English), or \"auto\" (automatic detection).",
                "enum": [
                  "zh",
                  "en",
                  "auto"
                ]
              },
              "save_raw_content": {
                "type": "BOOLEAN",
                "description": "Whether to save the raw extracted content to a local file. Defaults to false."
              }
            },
            "required": [
              "url"
            ]
          }
        },
        {
          "name": "sii_cognitions",
          "description": "Access and search cognitions from SII backend. Can list cognitions with various filters or get detailed information about specific cognitions. Only available when authenticated with SII.",
          "parameters": {
            "type": "OBJECT",
            "properties": {
              "action": {
                "type": "STRING",
                "description": "The action to perform: \"list\" for getting cognitions list, \"detail\" for getting specific cognition details.",
                "enum": [
                  "list",
                  "detail"
                ]
              },
              "search": {
                "type": "STRING",
                "description": "Search keyword for filtering cognitions (only for \"list\" action)."
              },
              "skip": {
                "type": "NUMBER",
                "description": "Number of records to skip for pagination (only for \"list\" action, default: 0)."
              },
              "limit": {
                "type": "NUMBER",
                "description": "Number of results to return (only for \"list\" action, default: 20, max: 100)."
              },
              "time_filter": {
                "type": "STRING",
                "description": "Time filter: \"day\", \"week\", \"month\", \"all\" (only for \"list\" action, default: \"day\").",
                "enum": [
                  "day",
                  "week",
                  "month",
                  "all"
                ]
              },
              "sort": {
                "type": "STRING",
                "description": "Sort method: \"latest\", \"hot\", \"most_liked\" (only for \"list\" action, default: \"latest\").",
                "enum": [
                  "latest",
                  "hot",
                  "most_liked"
                ]
              },
              "topics": {
                "type": "ARRAY",
                "description": "Topics to filter by (only for \"list\" action).",
                "items": {
                  "type": "STRING"
                }
              },
              "source_filter": {
                "type": "STRING",
                "description": "Source filter (only for \"list\" action, default: \"all\")."
              },
              "author_filter": {
                "type": "STRING",
                "description": "Author filter (only for \"list\" action)."
              },
              "cognition_id": {
                "type": "STRING",
                "description": "Cognition ID for getting details (only for \"detail\" action)."
              }
            },
            "required": [
              "action"
            ]
          }
        },
        {
          "name": "sii_deep_research",
          "description": "Conducts comprehensive research using the Engineer Deep Research API. Features include: auto-start research sessions with milestone tracking, real-time phase analysis, session history with organized milestones, phase-specific information extraction, and service health monitoring. Supports both Chinese and English research outputs. Only available when authenticated with SII.",
          "parameters": {
            "type": "OBJECT",
            "properties": {
              "research_question": {
                "type": "STRING",
                "description": "The research question to investigate thoroughly. Required for \"start\" action."
              },
              "language": {
                "type": "STRING",
                "description": "Language for the research output (zh for Chinese, en for English). Default: zh.",
                "enum": [
                  "zh",
                  "en"
                ]
              },
              "research_params": {
                "type": "OBJECT",
                "description": "Research configuration parameters for auto_start.",
                "properties": {
                  "web_search_enabled": {
                    "type": "BOOLEAN",
                    "description": "Enable web search during research. Default: true."
                  },
                  "cognition_search_enabled": {
                    "type": "BOOLEAN",
                    "description": "Enable cognition database search. Default: true."
                  },
                  "max_iterations": {
                    "type": "NUMBER",
                    "description": "Maximum number of research iterations. Default: 2."
                  }
                }
              },
              "action": {
                "type": "STRING",
                "description": "Action to perform: start (create and auto-start new research), health (check service health), history (get session history), phase_analysis (analyze current phase), extract_phase_info (extract phase-specific information), monitor (monitor research progress with milestone tracking), milestone_summary (get quick milestone summary with content previews), save_results (save complete research results to local file when research is completed).",
                "enum": [
                  "start",
                  "health",
                  "history",
                  "phase_analysis",
                  "extract_phase_info",
                  "monitor",
                  "milestone_summary",
                  "save_results"
                ]
              },
              "session_id": {
                "type": "STRING",
                "description": "Session ID for operations that require an existing session. Required for: history, phase_analysis, extract_phase_info actions."
              },
              "target_phase": {
                "type": "STRING",
                "description": "Target phase for extract_phase_info action. Required when action is \"extract_phase_info\".",
                "enum": [
                  "planning",
                  "web_search",
                  "browsing",
                  "writing",
                  "completion",
                  "session_end"
                ]
              },
              "limit": {
                "type": "NUMBER",
                "description": "Limit for history retrieval (default: 100, max: 1000)."
              },
              "metadata": {
                "type": "OBJECT",
                "description": "Custom metadata for the research session. Used when creating new sessions.",
                "properties": {
                  "description": {
                    "type": "STRING",
                    "description": "Description of the research session."
                  },
                  "created_by": {
                    "type": "STRING",
                    "description": "Creator of the research session."
                  }
                }
              }
            },
            "required": []
          }
        },
        {
          "name": "sii_email_notification",
          "description": "Send important email notifications to the authenticated user via SII. Only use when the user explicitly requests or indicates remote monitoring AND an important event occurs (critical failure, long-running job completed, or action required). Avoid frequent notifications. Include full context so the user needs no follow-up to understand the situation.",
          "parameters": {
            "type": "OBJECT",
            "properties": {
              "user_requested": {
                "type": "BOOLEAN",
                "description": "Whether the user explicitly requested an email notification for this task/session."
              },
              "user_remote_monitoring": {
                "type": "BOOLEAN",
                "description": "Whether the user indicated they are away and need remote monitoring notifications."
              },
              "event": {
                "type": "STRING",
                "enum": [
                  "critical_failure",
                  "long_running_completed",
                  "need_user_action"
                ],
                "description": "The event that triggers the notification. Email will be sent only when preconditions are satisfied."
              },
              "subject": {
                "type": "STRING",
                "description": "Optional email subject. If omitted, a suitable subject will be generated from the event."
              },
              "raw_content": {
                "type": "STRING",
                "description": "Optional raw content to embed directly into the email. If provided, structured fields are ignored."
              },
              "background": {
                "type": "STRING",
                "description": "Background and cause."
              },
              "current_status": {
                "type": "STRING",
                "description": "Current state."
              },
              "result_summary": {
                "type": "STRING",
                "description": "Result summary (success/failure and key metrics)."
              },
              "errors": {
                "type": "ARRAY",
                "items": {
                  "type": "STRING"
                },
                "description": "Specific error details if any."
              },
              "next_steps": {
                "type": "ARRAY",
                "items": {
                  "type": "STRING"
                },
                "description": "Suggested next actions for the user (if applicable)."
              },
              "sender_name": {
                "type": "STRING",
                "description": "Optional sender display name. Default: \"SII CLI Agent\"."
              }
            },
            "required": [
              "event"
            ]
          }
        },
        {
          "name": "sii_hybrid_search",
          "description": "Performs intelligent hybrid search combining web search and cognitions from SII backend. Use separate queries for different sources: natural language for web search, keywords for cognition search. Features advanced relevance scoring, comprehensive filtering options, and intelligent result merging with source diversity.",
          "parameters": {
            "type": "OBJECT",
            "properties": {
              "web_query": {
                "type": "STRING",
                "description": "Natural language query for web search (e.g., \"latest artificial intelligence developments\"). Use descriptive, conversational language. If not provided, web search will be skipped."
              },
              "cognition_query": {
                "type": "STRING",
                "description": "Space-separated keywords for cognition search (e.g., \"artificial intelligence development trends\"). Use specific keywords separated by spaces. If not provided, cognition search will be skipped."
              },
              "max_web_results": {
                "type": "NUMBER",
                "description": "Maximum number of web search results to fetch (default: 5, max: 10)."
              },
              "max_cognition_results": {
                "type": "NUMBER",
                "description": "Maximum number of cognition results to fetch (default: 10, max: 20)."
              },
              "cognition_time_filter": {
                "type": "STRING",
                "description": "Time filter for cognitions: \"day\", \"week\", \"month\", \"all\" (default: \"all\").",
                "enum": [
                  "day",
                  "week",
                  "month",
                  "all"
                ]
              },
              "cognition_sort": {
                "type": "STRING",
                "description": "Sort method for cognitions: \"latest\", \"hot\", \"most_liked\" (default: \"hot\").",
                "enum": [
                  "latest",
                  "hot",
                  "most_liked"
                ]
              },
              "cognition_topics": {
                "type": "ARRAY",
                "description": "Topics to filter cognitions by (e.g., [\"AI\", \"Technology\"]).",
                "items": {
                  "type": "STRING"
                }
              },
              "cognition_source_filter": {
                "type": "STRING",
                "description": "Source filter for cognitions (e.g., \"arxiv\", \"reddit\", \"twitter\")."
              },
              "cognition_author_filter": {
                "type": "STRING",
                "description": "Author filter for cognitions."
              },
              "result_limit": {
                "type": "NUMBER",
                "description": "Maximum number of final merged results to return (default: 15, max: 30)."
              }
            },
            "required": []
          }
        },
        {
          "name": "sii_web_search",
          "description": "Performs a web search using SII backend search service and returns the results. This tool is useful for finding information on the internet based on a query.",
          "parameters": {
            "type": "OBJECT",
            "properties": {
              "query": {
                "type": "STRING",
                "description": "The search query to find information on the web."
              }
            },
            "required": [
              "query"
            ]
          }
        },
        {
          "name": "sii_web_fetch",
          "description": "Processes content from URL(s), including local and private network addresses (e.g., localhost), embedded in a prompt using SII backend service. Include up to 20 URLs and instructions (e.g., summarize, extract specific data) directly in the 'prompt' parameter.",
          "parameters": {
            "properties": {
              "prompt": {
                "description": "A comprehensive prompt that includes the URL(s) (up to 20) to fetch and specific instructions on how to process their content (e.g., \"Summarize https://example.com/article and extract key points from https://another.com/data\"). Must contain as least one URL starting with http:// or https://.",
                "type": "STRING"
              }
            },
            "required": [
              "prompt"
            ],
            "type": "OBJECT"
          }
        },
        {
          "name": "write_file",
          "description": "Writes content to a specified file in the local filesystem.\n\n      The user has the ability to modify `content`. If modified, this will be stated in the response.",
          "parameters": {
            "properties": {
              "file_path": {
                "description": "The absolute path to the file to write to (e.g., '/home/user/project/file.txt'). Relative paths are not supported.",
                "type": "STRING"
              },
              "content": {
                "description": "The content to write to the file.",
                "type": "STRING"
              }
            },
            "required": [
              "file_path",
              "content"
            ],
            "type": "OBJECT"
          }
        }
      ],
      "session_metadata": {
        "working_directory": "/data2/jdsu/projects/OmniAvatar",
        "cli_version": "unknown",
        "platform": "linux"
      }
    }
  }
]